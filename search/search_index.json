{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"keras-vis: Keras Visualization Toolkit keras-vis\u306fKeras\u30e2\u30c7\u30eb\u306e\u53ef\u8996\u5316\u3068\u30c7\u30d0\u30c3\u30b0\u306e\u305f\u3081\u306e\u30cf\u30a4\u30ec\u30d9\u30eb\u30c4\u30fc\u30eb\u30ad\u30c3\u30c8\u3067\u3059\u3002 \u73fe\u5728\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u53ef\u8996\u5316: Activation maximization Saliency maps Class activation maps \u5168\u3066\u306e\u53ef\u8996\u5316\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067N\u6b21\u5143\u753b\u50cf\u5165\u529b\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\u3059\u306a\u308f\u3061\u3001\u30e2\u30c7\u30eb\u3078\u306eN\u6b21\u5143\u753b\u50cf\u5165\u529b\u3092\u4e00\u822c\u5316\u3057\u3066\u3044\u307e\u3059\u3002 keras-vis\u306f\u7c21\u6f54\u3067\u4f7f\u3044\u3084\u3059\u304f\u3001\u62e1\u5f35\u53ef\u80fd\u306a\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u3092\u5099\u3048\u3066\u3044\u307e\u3059\u3002\u4e0a\u8a18\u306e\u53ef\u8996\u5316\u5168\u3066\u3092\u30a8\u30cd\u30eb\u30ae\u30fc\u6700\u5c0f\u5316\u554f\u984c\u3068\u3057\u3066\u4e00\u822c\u5316\u3057\u3066\u3044\u307e\u3059\u3002 theano\u3068tensorflow\u4e21\u65b9\u306e\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3068\u4e92\u63db\u6027\u304c\u3042\u308a\u3001'channels_first'\u3068'channels_last'\u4e21\u65b9\u306e\u30c7\u30fc\u30bf\u5f62\u5f0f\u3092\u6271\u3048\u307e\u3059\u3002 \u30af\u30a4\u30c3\u30af\u30ea\u30f3\u30af https://keisen.github.io/keras-vis-docs-ja \u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8aad\u3093\u3067\u304f\u3060\u3055\u3044\u3002 \u8cea\u554f\u3084\u8b70\u8ad6\u306e\u305f\u3081\u306bSlack\u306e \u30c1\u30e3\u30f3\u30cd\u30eb \u306b\u53c2\u52a0\u3057\u3066\u4e0b\u3055\u3044\u3002 \u79c1\u305f\u3061\u306f waffle.io \u3067\u65b0\u3057\u3044\u6a5f\u80fd\u3084\u30bf\u30b9\u30af\u3092\u8ffd\u8de1\u3057\u3066\u3044\u307e\u3059\u3002PullRequest\u3067\u5354\u529b\u3057\u3066\u3082\u3089\u3048\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002 Getting Started \u753b\u50cf\u306e\u8aa4\u5dee\u9006\u4f1d\u642c\u554f\u984c\u306f\u3001\u640d\u5931\u95a2\u6570\u3092\u6700\u5c0f\u306b\u3059\u308b\u5165\u529b\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u30b4\u30fc\u30eb\u3067\u3059\u3002\u753b\u50cf\u306e\u8aa4\u5dee\u9006\u4f1d\u642c\u554f\u984c\u306e\u8a2d\u5b9a\u306f\u7c21\u5358\u3067\u3059\u3002 \u91cd\u307f\u4ed8\u304d\u640d\u5931\u95a2\u6570\u3092\u5b9a\u7fa9 \u69d8\u3005\u306a\u6709\u7528\u306a\u640d\u5931\u95a2\u6570\u304c losses \u3067\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002 \u30ab\u30b9\u30bf\u30e0\u640d\u5931\u95a2\u6570\u306f Loss.build_loss \u3092\u5b9f\u88c5\u3059\u308b\u3053\u3068\u3067\u5b9a\u7fa9\u3067\u304d\u307e\u3059\u3002 from vis.losses import ActivationMaximization from vis.regularizers import TotalVariation, LPNorm filter_indices = [1, 2, 3] # \u30bf\u30d7\u30eb\u306f(\u640d\u5931\u95a2\u6570, \u91cd\u307f\u4fc2\u6570)\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002 # \u5fc5\u8981\u306b\u5fdc\u3058\u3066\u6b63\u5247\u5316(regularizers)\u3082\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002 losses = [ (ActivationMaximization(keras_layer, filter_indices), 1), (LPNorm(model.input), 10), (TotalVariation(model.input), 10) ] \u91cd\u307f\u4ed8\u304d\u640d\u5931\u3092\u6700\u5c0f\u5316\u3059\u308b\u305f\u3081\u306e\u6700\u9069\u5316\u8a2d\u5b9a \u81ea\u7136\u306a\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u305f\u3081\u306b\u3001\u753b\u50cf\u63a2\u7d22\u7a7a\u9593\u306f\u6b63\u5247\u5316\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u7528\u3044\u3066\u5236\u9650\u3055\u308c\u307e\u3059\u3002 \u3044\u304f\u3064\u304b\u306e\u6c4e\u7528\u7684\u306a\u6b63\u5247\u5316\u306f regularizers \u306b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u640d\u5931\u95a2\u6570\u306e\u3088\u3046\u306b\u3001 Loss.build_loss \u3092\u5b9f\u88c5\u3059\u308b\u3053\u3068\u3067\u30ab\u30b9\u30bf\u30e0\u6b63\u5247\u5316\u3092\u5b9a\u7fa9\u3067\u304d\u307e\u3059\u3002 from vis.optimizer import Optimizer optimizer = Optimizer(model.input, losses) opt_img, grads, _ = optimizer.minimize() \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u53ef\u8996\u5316\u306e\u5177\u4f53\u4f8b\u306f examples folder \u306b\u3042\u308a\u307e\u3059\u3002 Note \u73fe\u5728PyPI\u306b\u767b\u9332\u3055\u308c\u3066\u3044\u308bkeras-vis\u306f\u53e4\u3044\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u3001\u591a\u304f\u306e\u4e0d\u5177\u5408\u3092\u542b\u307f\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u307e\u305f\u306fGitHub\u7d4c\u7531\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002 1) theano\u307e\u305f\u306ftensorflow\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3068\u5171\u306b keras \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 Keras\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c2.0\u4ee5\u4e0a\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u4e0b\u3055\u3044\u3002 2) keras-vis\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo python setup.py install GitHub\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo pip install git+https://github.com/raghakot/keras-vis.git PyPI\u30d1\u30c3\u30b1\u30fc\u30b8\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo pip install keras-vis Note \u73fe\u5728\u3001\u30ea\u30f3\u30af\u306f\u58ca\u308c\u3066\u304a\u308a\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u5168\u4f53\u3092\u4fee\u6b63\u4e2d\u3067\u3059\u3002 \u30b5\u30f3\u30d7\u30eb\u306f examples/ \u914d\u4e0b\u3092\u53c2\u7167\u3057\u3066\u4e0b\u3055\u3044\u3002 \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306f\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u3067\u3059\u3002\u8fd1\u5e74\u3067\u306f\u3001\u7573\u307f\u8fbc\u307f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7406\u89e3\u3057\u53ef\u8996\u5316\u3059\u308b\u305f\u3081\u306e\u3001\u3044\u304f\u3064\u304b\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u958b\u767a\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u305d\u308c\u3089\u306e\u8ad6\u6587\u306f\u79c1\u9054\u306b\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u306b\u8033\u3092\u50be\u3051\u3001\u8aa4\u5206\u985e\u3092\u8a3a\u65ad\u3057\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u904e\u5b66\u7fd2\uff08\u307e\u305f\u306f\u672a\u5b66\u7fd2\uff09\u304b\u3069\u3046\u304b\u3092\u8a55\u4fa1\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002 Guided-backprop\u3092\u4f7f\u3063\u3066\u6210\u9577\u3057\u3066\u3044\u308b\u4ed6\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30ea\u30b9\u30c8\u306e\u4e2d\u3067\u3001 trippy art \u3001 neural style transfer (texture style transfer)\u3092\u4f5c\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3055\u307e\u3056\u307e\u306a\u8996\u899a\u5316\u304c\u3001\u305d\u308c\u305e\u308c\u306e\u30da\u30fc\u30b8\u3067\u6587\u66f8\u5316\u3055\u308c\u3001\u3053\u3053\u306b\u8981\u7d04\u3055\u308c\u3066\u3044\u307e\u3059\u3002 Conv filter visualization \u7573\u307f\u8fbc\u307f\u30d5\u30a3\u30eb\u30bf\u306f\u3001\u985e\u4f3c\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d1\u30bf\u30fc\u30f3\u304c\u5165\u529b\u753b\u50cf\u5185\u306b\u898b\u3089\u308c\u308b\u3068\u304d\u3001\u51fa\u529b\u3092\u6700\u5927\u5316\u3059\u308b'\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30de\u30c3\u30c1\u30f3\u30b0'\u30d5\u30a3\u30eb\u30bf\u3092\u5b66\u7fd2\u3057\u307e\u3059\u3002 \u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u6700\u5927\u5316\u306b\u3088\u3063\u3066\u3053\u308c\u3089\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3092\u8996\u899a\u5316\u3057\u307e\u3059\u3002 Dense layer visualization \u79c1\u305f\u3061\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u904e\u5b66\u7fd2\uff08\u307e\u305f\u306f\u672a\u5b66\u7fd2\uff09\u304b\u3001\u4e00\u822c\u5316\u304c\u9069\u5207\u304b\u3069\u3046\u304b\u3092\u3069\u306e\u3088\u3046\u306b\u8a55\u4fa1\u3067\u304d\u307e\u3059\u304b\uff1f Attention Maps \u79c1\u305f\u3061\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u305d\u306e\u51fa\u529b\u3092\u6c7a\u5b9a\u3059\u308b\u969b\u3001\u753b\u50cf\u306e\u6b63\u3057\u3044\u90e8\u5206\u3092\u6ce8\u8996\u3057\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u3069\u306e\u3088\u3046\u306b\u8a55\u4fa1\u3067\u304d\u307e\u3059\u304b\uff1f \u6700\u9069\u5316\u306e\u9032\u6357\u3092GIF\u30a2\u30cb\u30e1\u3067\u51fa\u529b\u3059\u308b callbacks \u3092\u6d3b\u7528\u3057\u3066\u6700\u9069\u5316\u306e\u9032\u6357\u3092GIF\u30a2\u30cb\u30e1\u3068\u3057\u3066\u751f\u6210\u53ef\u80fd\u3067\u3059\u3002 \u6b21\u306e\u4f8b\u306f\u3001'\u30aa\u30bc\u30eb'\u30af\u30e9\u30b9\uff08output_index: 20\uff09\u306eActivation maximization\u306b\u3088\u308b\u53ef\u8996\u5316\u3067\u3059\u3002 from vis.losses import ActivationMaximization from vis.regularizers import TotalVariation, LPNorm from vis.modifiers import Jitter from vis.optimizer import Optimizer from vis.callbacks import GifGenerator from vis.utils.vggnet import VGG16 # VGG16\u3092ImageNet\u306eWeights\u3067\u69cb\u7bc9\u3059\u308b model = VGG16(weights='imagenet', include_top=True) print('Model loaded.') # \u53ef\u8996\u5316\u3057\u305f\u3044\u30ec\u30a4\u30e4\u306e\u540d\u524d\uff08vggnet.py\u306e\u30e2\u30c7\u30eb\u5b9a\u7fa9\u3092\u53c2\u7167\u3057\u3066\u4e0b\u3055\u3044\uff09 layer_name = 'predictions' layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]]) output_class = [20] losses = [ (ActivationMaximization(layer_dict[layer_name], output_class), 2), (LPNorm(model.input), 10), (TotalVariation(model.input), 10) ] opt = Optimizer(model.input, losses) opt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')]) \u51fa\u529b\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u304b\u6ce8\u76ee\u3057\u3066\u4e0b\u3055\u3044\u3002\u3000\u3053\u308c\u306f\u9bae\u660e\u306a\u6d3b\u6027\u5316\u6700\u5927\u5316\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u308b[ImageModifier]\u306e\u4e00\u7a2e\u3067\u3042\u308b Jitter \u3092\u4f7f\u3063\u3066\u3044\u308b\u304b\u3089\u3067\u3059\u3002 \u7df4\u7fd2\u3068\u3057\u3066\u6b21\u306b\u6311\u6226\u3057\u3066\u4e0b\u3055\u3044\u3002 Jitter\u3092\u53d6\u308a\u9664\u304f \u640d\u5931\u306e\u91cd\u307f\u3092\u5909\u5316\u3055\u305b\u308b Citation \u3082\u3057keras-vis\u304c\u3042\u306a\u305f\u306e\u7814\u7a76\u306e\u5f79\u306b\u305f\u3063\u305f\u3089\u3001\u3042\u306a\u305f\u306e\u51fa\u7248\u7269\u306b\u5f15\u7528\u3057\u3066\u4e0b\u3055\u3044\u3002\u3053\u3053\u306bBibTeX\u30a8\u30f3\u30c8\u30ea\u306e\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002 @misc{raghakotkerasvis, title={keras-vis}, author={Kotikalapudi, Raghavendra and contributors}, year={2017}, publisher={GitHub}, howpublished={\\url{https://github.com/raghakot/keras-vis}}, }","title":"Home"},{"location":"#keras-vis-keras-visualization-toolkit","text":"keras-vis\u306fKeras\u30e2\u30c7\u30eb\u306e\u53ef\u8996\u5316\u3068\u30c7\u30d0\u30c3\u30b0\u306e\u305f\u3081\u306e\u30cf\u30a4\u30ec\u30d9\u30eb\u30c4\u30fc\u30eb\u30ad\u30c3\u30c8\u3067\u3059\u3002 \u73fe\u5728\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u53ef\u8996\u5316: Activation maximization Saliency maps Class activation maps \u5168\u3066\u306e\u53ef\u8996\u5316\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067N\u6b21\u5143\u753b\u50cf\u5165\u529b\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\u3059\u306a\u308f\u3061\u3001\u30e2\u30c7\u30eb\u3078\u306eN\u6b21\u5143\u753b\u50cf\u5165\u529b\u3092\u4e00\u822c\u5316\u3057\u3066\u3044\u307e\u3059\u3002 keras-vis\u306f\u7c21\u6f54\u3067\u4f7f\u3044\u3084\u3059\u304f\u3001\u62e1\u5f35\u53ef\u80fd\u306a\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u3092\u5099\u3048\u3066\u3044\u307e\u3059\u3002\u4e0a\u8a18\u306e\u53ef\u8996\u5316\u5168\u3066\u3092\u30a8\u30cd\u30eb\u30ae\u30fc\u6700\u5c0f\u5316\u554f\u984c\u3068\u3057\u3066\u4e00\u822c\u5316\u3057\u3066\u3044\u307e\u3059\u3002 theano\u3068tensorflow\u4e21\u65b9\u306e\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3068\u4e92\u63db\u6027\u304c\u3042\u308a\u3001'channels_first'\u3068'channels_last'\u4e21\u65b9\u306e\u30c7\u30fc\u30bf\u5f62\u5f0f\u3092\u6271\u3048\u307e\u3059\u3002","title":"keras-vis: Keras Visualization Toolkit"},{"location":"#_1","text":"https://keisen.github.io/keras-vis-docs-ja \u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8aad\u3093\u3067\u304f\u3060\u3055\u3044\u3002 \u8cea\u554f\u3084\u8b70\u8ad6\u306e\u305f\u3081\u306bSlack\u306e \u30c1\u30e3\u30f3\u30cd\u30eb \u306b\u53c2\u52a0\u3057\u3066\u4e0b\u3055\u3044\u3002 \u79c1\u305f\u3061\u306f waffle.io \u3067\u65b0\u3057\u3044\u6a5f\u80fd\u3084\u30bf\u30b9\u30af\u3092\u8ffd\u8de1\u3057\u3066\u3044\u307e\u3059\u3002PullRequest\u3067\u5354\u529b\u3057\u3066\u3082\u3089\u3048\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002","title":"\u30af\u30a4\u30c3\u30af\u30ea\u30f3\u30af"},{"location":"#getting-started","text":"\u753b\u50cf\u306e\u8aa4\u5dee\u9006\u4f1d\u642c\u554f\u984c\u306f\u3001\u640d\u5931\u95a2\u6570\u3092\u6700\u5c0f\u306b\u3059\u308b\u5165\u529b\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u30b4\u30fc\u30eb\u3067\u3059\u3002\u753b\u50cf\u306e\u8aa4\u5dee\u9006\u4f1d\u642c\u554f\u984c\u306e\u8a2d\u5b9a\u306f\u7c21\u5358\u3067\u3059\u3002 \u91cd\u307f\u4ed8\u304d\u640d\u5931\u95a2\u6570\u3092\u5b9a\u7fa9 \u69d8\u3005\u306a\u6709\u7528\u306a\u640d\u5931\u95a2\u6570\u304c losses \u3067\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002 \u30ab\u30b9\u30bf\u30e0\u640d\u5931\u95a2\u6570\u306f Loss.build_loss \u3092\u5b9f\u88c5\u3059\u308b\u3053\u3068\u3067\u5b9a\u7fa9\u3067\u304d\u307e\u3059\u3002 from vis.losses import ActivationMaximization from vis.regularizers import TotalVariation, LPNorm filter_indices = [1, 2, 3] # \u30bf\u30d7\u30eb\u306f(\u640d\u5931\u95a2\u6570, \u91cd\u307f\u4fc2\u6570)\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002 # \u5fc5\u8981\u306b\u5fdc\u3058\u3066\u6b63\u5247\u5316(regularizers)\u3082\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002 losses = [ (ActivationMaximization(keras_layer, filter_indices), 1), (LPNorm(model.input), 10), (TotalVariation(model.input), 10) ] \u91cd\u307f\u4ed8\u304d\u640d\u5931\u3092\u6700\u5c0f\u5316\u3059\u308b\u305f\u3081\u306e\u6700\u9069\u5316\u8a2d\u5b9a \u81ea\u7136\u306a\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u305f\u3081\u306b\u3001\u753b\u50cf\u63a2\u7d22\u7a7a\u9593\u306f\u6b63\u5247\u5316\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u7528\u3044\u3066\u5236\u9650\u3055\u308c\u307e\u3059\u3002 \u3044\u304f\u3064\u304b\u306e\u6c4e\u7528\u7684\u306a\u6b63\u5247\u5316\u306f regularizers \u306b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u640d\u5931\u95a2\u6570\u306e\u3088\u3046\u306b\u3001 Loss.build_loss \u3092\u5b9f\u88c5\u3059\u308b\u3053\u3068\u3067\u30ab\u30b9\u30bf\u30e0\u6b63\u5247\u5316\u3092\u5b9a\u7fa9\u3067\u304d\u307e\u3059\u3002 from vis.optimizer import Optimizer optimizer = Optimizer(model.input, losses) opt_img, grads, _ = optimizer.minimize() \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u53ef\u8996\u5316\u306e\u5177\u4f53\u4f8b\u306f examples folder \u306b\u3042\u308a\u307e\u3059\u3002 Note \u73fe\u5728PyPI\u306b\u767b\u9332\u3055\u308c\u3066\u3044\u308bkeras-vis\u306f\u53e4\u3044\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u3001\u591a\u304f\u306e\u4e0d\u5177\u5408\u3092\u542b\u307f\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u307e\u305f\u306fGitHub\u7d4c\u7531\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002 1) theano\u307e\u305f\u306ftensorflow\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3068\u5171\u306b keras \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 Keras\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c2.0\u4ee5\u4e0a\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u4e0b\u3055\u3044\u3002 2) keras-vis\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo python setup.py install GitHub\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo pip install git+https://github.com/raghakot/keras-vis.git PyPI\u30d1\u30c3\u30b1\u30fc\u30b8\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo pip install keras-vis Note \u73fe\u5728\u3001\u30ea\u30f3\u30af\u306f\u58ca\u308c\u3066\u304a\u308a\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u5168\u4f53\u3092\u4fee\u6b63\u4e2d\u3067\u3059\u3002 \u30b5\u30f3\u30d7\u30eb\u306f examples/ \u914d\u4e0b\u3092\u53c2\u7167\u3057\u3066\u4e0b\u3055\u3044\u3002 \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306f\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u3067\u3059\u3002\u8fd1\u5e74\u3067\u306f\u3001\u7573\u307f\u8fbc\u307f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7406\u89e3\u3057\u53ef\u8996\u5316\u3059\u308b\u305f\u3081\u306e\u3001\u3044\u304f\u3064\u304b\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u958b\u767a\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u305d\u308c\u3089\u306e\u8ad6\u6587\u306f\u79c1\u9054\u306b\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u306b\u8033\u3092\u50be\u3051\u3001\u8aa4\u5206\u985e\u3092\u8a3a\u65ad\u3057\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u904e\u5b66\u7fd2\uff08\u307e\u305f\u306f\u672a\u5b66\u7fd2\uff09\u304b\u3069\u3046\u304b\u3092\u8a55\u4fa1\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002 Guided-backprop\u3092\u4f7f\u3063\u3066\u6210\u9577\u3057\u3066\u3044\u308b\u4ed6\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30ea\u30b9\u30c8\u306e\u4e2d\u3067\u3001 trippy art \u3001 neural style transfer (texture style transfer)\u3092\u4f5c\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3055\u307e\u3056\u307e\u306a\u8996\u899a\u5316\u304c\u3001\u305d\u308c\u305e\u308c\u306e\u30da\u30fc\u30b8\u3067\u6587\u66f8\u5316\u3055\u308c\u3001\u3053\u3053\u306b\u8981\u7d04\u3055\u308c\u3066\u3044\u307e\u3059\u3002","title":"Getting Started"},{"location":"#conv-filter-visualization","text":"\u7573\u307f\u8fbc\u307f\u30d5\u30a3\u30eb\u30bf\u306f\u3001\u985e\u4f3c\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d1\u30bf\u30fc\u30f3\u304c\u5165\u529b\u753b\u50cf\u5185\u306b\u898b\u3089\u308c\u308b\u3068\u304d\u3001\u51fa\u529b\u3092\u6700\u5927\u5316\u3059\u308b'\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30de\u30c3\u30c1\u30f3\u30b0'\u30d5\u30a3\u30eb\u30bf\u3092\u5b66\u7fd2\u3057\u307e\u3059\u3002 \u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u6700\u5927\u5316\u306b\u3088\u3063\u3066\u3053\u308c\u3089\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3092\u8996\u899a\u5316\u3057\u307e\u3059\u3002","title":"Conv filter visualization"},{"location":"#dense-layer-visualization","text":"\u79c1\u305f\u3061\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u904e\u5b66\u7fd2\uff08\u307e\u305f\u306f\u672a\u5b66\u7fd2\uff09\u304b\u3001\u4e00\u822c\u5316\u304c\u9069\u5207\u304b\u3069\u3046\u304b\u3092\u3069\u306e\u3088\u3046\u306b\u8a55\u4fa1\u3067\u304d\u307e\u3059\u304b\uff1f","title":"Dense layer visualization"},{"location":"#attention-maps","text":"\u79c1\u305f\u3061\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u305d\u306e\u51fa\u529b\u3092\u6c7a\u5b9a\u3059\u308b\u969b\u3001\u753b\u50cf\u306e\u6b63\u3057\u3044\u90e8\u5206\u3092\u6ce8\u8996\u3057\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u3069\u306e\u3088\u3046\u306b\u8a55\u4fa1\u3067\u304d\u307e\u3059\u304b\uff1f","title":"Attention Maps"},{"location":"#gif","text":"callbacks \u3092\u6d3b\u7528\u3057\u3066\u6700\u9069\u5316\u306e\u9032\u6357\u3092GIF\u30a2\u30cb\u30e1\u3068\u3057\u3066\u751f\u6210\u53ef\u80fd\u3067\u3059\u3002 \u6b21\u306e\u4f8b\u306f\u3001'\u30aa\u30bc\u30eb'\u30af\u30e9\u30b9\uff08output_index: 20\uff09\u306eActivation maximization\u306b\u3088\u308b\u53ef\u8996\u5316\u3067\u3059\u3002 from vis.losses import ActivationMaximization from vis.regularizers import TotalVariation, LPNorm from vis.modifiers import Jitter from vis.optimizer import Optimizer from vis.callbacks import GifGenerator from vis.utils.vggnet import VGG16 # VGG16\u3092ImageNet\u306eWeights\u3067\u69cb\u7bc9\u3059\u308b model = VGG16(weights='imagenet', include_top=True) print('Model loaded.') # \u53ef\u8996\u5316\u3057\u305f\u3044\u30ec\u30a4\u30e4\u306e\u540d\u524d\uff08vggnet.py\u306e\u30e2\u30c7\u30eb\u5b9a\u7fa9\u3092\u53c2\u7167\u3057\u3066\u4e0b\u3055\u3044\uff09 layer_name = 'predictions' layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]]) output_class = [20] losses = [ (ActivationMaximization(layer_dict[layer_name], output_class), 2), (LPNorm(model.input), 10), (TotalVariation(model.input), 10) ] opt = Optimizer(model.input, losses) opt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')]) \u51fa\u529b\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u304b\u6ce8\u76ee\u3057\u3066\u4e0b\u3055\u3044\u3002\u3000\u3053\u308c\u306f\u9bae\u660e\u306a\u6d3b\u6027\u5316\u6700\u5927\u5316\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u308b[ImageModifier]\u306e\u4e00\u7a2e\u3067\u3042\u308b Jitter \u3092\u4f7f\u3063\u3066\u3044\u308b\u304b\u3089\u3067\u3059\u3002 \u7df4\u7fd2\u3068\u3057\u3066\u6b21\u306b\u6311\u6226\u3057\u3066\u4e0b\u3055\u3044\u3002 Jitter\u3092\u53d6\u308a\u9664\u304f \u640d\u5931\u306e\u91cd\u307f\u3092\u5909\u5316\u3055\u305b\u308b","title":"\u6700\u9069\u5316\u306e\u9032\u6357\u3092GIF\u30a2\u30cb\u30e1\u3067\u51fa\u529b\u3059\u308b"},{"location":"#citation","text":"\u3082\u3057keras-vis\u304c\u3042\u306a\u305f\u306e\u7814\u7a76\u306e\u5f79\u306b\u305f\u3063\u305f\u3089\u3001\u3042\u306a\u305f\u306e\u51fa\u7248\u7269\u306b\u5f15\u7528\u3057\u3066\u4e0b\u3055\u3044\u3002\u3053\u3053\u306bBibTeX\u30a8\u30f3\u30c8\u30ea\u306e\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002 @misc{raghakotkerasvis, title={keras-vis}, author={Kotikalapudi, Raghavendra and contributors}, year={2017}, publisher={GitHub}, howpublished={\\url{https://github.com/raghakot/keras-vis}}, }","title":"Citation"},{"location":"vis.backend/","text":"Source: vis/backend/ init .py#L0 Global Variables name modify_model_backprop modify_model_backprop(model, backprop_modifier) \u30ab\u30b9\u30bf\u30e0OP\u3092\u4f7f\u7528\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u64ad\u306e\u52d5\u4f5c\u3092\u5909\u66f4\u3059\u308b\u305f\u3081\u306b\u5168\u3066\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092\u5909\u66f4\u3057\u3066\u30e2\u30c7\u30eb\u306e\u30b3\u30d4\u30fc\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 \u5f15\u6570: model : keras.models.Model \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 backprop_modifier : {'guided', 'rectified'} \u306e\u3044\u305a\u308c\u304b\uff11\u3064 \u623b\u308a\u5024: \u6d3b\u6027\u5316\u95a2\u6570\u3092\u5909\u66f4\u3057\u305f\u30e2\u30c7\u30eb\u306e\u30b3\u30d4\u30fc set_random_seed set_random_seed(seed_value=1337) \u518d\u73fe\u6027\u3092\u4fdd\u6301\u3059\u308b\u305f\u3081\u306b\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 \u5f15\u6570: seed_value : \u30b7\u30fc\u30c9\u5024\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024=\u60aa\u540d\u9ad8\u30441337\uff09","title":"\u30d0\u30c3\u30af\u30a8\u30f3\u30c9"},{"location":"vis.backend/#global-variables","text":"name","title":"Global Variables"},{"location":"vis.backend/#modify_model_backprop","text":"modify_model_backprop(model, backprop_modifier) \u30ab\u30b9\u30bf\u30e0OP\u3092\u4f7f\u7528\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u64ad\u306e\u52d5\u4f5c\u3092\u5909\u66f4\u3059\u308b\u305f\u3081\u306b\u5168\u3066\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092\u5909\u66f4\u3057\u3066\u30e2\u30c7\u30eb\u306e\u30b3\u30d4\u30fc\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 \u5f15\u6570: model : keras.models.Model \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 backprop_modifier : {'guided', 'rectified'} \u306e\u3044\u305a\u308c\u304b\uff11\u3064 \u623b\u308a\u5024: \u6d3b\u6027\u5316\u95a2\u6570\u3092\u5909\u66f4\u3057\u305f\u30e2\u30c7\u30eb\u306e\u30b3\u30d4\u30fc","title":"modify_model_backprop"},{"location":"vis.backend/#set_random_seed","text":"set_random_seed(seed_value=1337) \u518d\u73fe\u6027\u3092\u4fdd\u6301\u3059\u308b\u305f\u3081\u306b\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 \u5f15\u6570: seed_value : \u30b7\u30fc\u30c9\u5024\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024=\u60aa\u540d\u9ad8\u30441337\uff09","title":"set_random_seed"},{"location":"vis.backprop_modifiers/","text":"Source: vis/backprop_modifiers.py#L0 guided guided(model) \u8aa4\u5dee\u9006\u4f1d\u64ad\u3092\u6b63\u306e\u6d3b\u6027\u5316\u306e\u305f\u3081\u306b\u6b63\u306e\u52fe\u914d\u3060\u3051\u4f1d\u64ad\u3059\u308b\u3088\u3046\u5909\u66f4\u3057\u307e\u3059\u3002 \u5f15\u6570: model : \u52fe\u914d\u8a08\u7b97\u306e\u4e0a\u66f8\u304d\u304c\u5fc5\u8981\u306a keras.models.Model \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 \u53c2\u7167: Guided back propagaton\u306e\u8a73\u7d30\u306f\u8ad6\u6587\u3067\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a [String For Simplicity: The All Convolutional Net] (https://arxiv.org/pdf/1412.6806.pdf) deconv, rectified, relu deconv, rectified, relu(model) \u6b63\u306e\u52fe\u914d\u306e\u307f\u3092\u4f1d\u642c\u3059\u308b\u3088\u3046\u8aa4\u5dee\u9006\u4f1d\u64ad\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 \u5f15\u6570: model : \u52fe\u914d\u8a08\u7b97\u306e\u4e0a\u66f8\u304d\u304c\u5fc5\u8981\u306a keras.models.Model \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 \u53c2\u7167: \u8a73\u7d30\u306f\u8ad6\u6587\u3067\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a [Visualizing and Understanding Convolutional Networks] (https://arxiv.org/pdf/1311.2901.pdf) get get(identifier)","title":"\u8aa4\u5dee\u9006\u4f1d\u64ad\u4fee\u98fe\u5b50"},{"location":"vis.backprop_modifiers/#guided","text":"guided(model) \u8aa4\u5dee\u9006\u4f1d\u64ad\u3092\u6b63\u306e\u6d3b\u6027\u5316\u306e\u305f\u3081\u306b\u6b63\u306e\u52fe\u914d\u3060\u3051\u4f1d\u64ad\u3059\u308b\u3088\u3046\u5909\u66f4\u3057\u307e\u3059\u3002 \u5f15\u6570: model : \u52fe\u914d\u8a08\u7b97\u306e\u4e0a\u66f8\u304d\u304c\u5fc5\u8981\u306a keras.models.Model \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 \u53c2\u7167: Guided back propagaton\u306e\u8a73\u7d30\u306f\u8ad6\u6587\u3067\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a [String For Simplicity: The All Convolutional Net] (https://arxiv.org/pdf/1412.6806.pdf)","title":"guided"},{"location":"vis.backprop_modifiers/#deconv-rectified-relu","text":"deconv, rectified, relu(model) \u6b63\u306e\u52fe\u914d\u306e\u307f\u3092\u4f1d\u642c\u3059\u308b\u3088\u3046\u8aa4\u5dee\u9006\u4f1d\u64ad\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 \u5f15\u6570: model : \u52fe\u914d\u8a08\u7b97\u306e\u4e0a\u66f8\u304d\u304c\u5fc5\u8981\u306a keras.models.Model \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 \u53c2\u7167: \u8a73\u7d30\u306f\u8ad6\u6587\u3067\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a [Visualizing and Understanding Convolutional Networks] (https://arxiv.org/pdf/1311.2901.pdf)","title":"deconv, rectified, relu"},{"location":"vis.backprop_modifiers/#get","text":"get(identifier)","title":"get"},{"location":"vis.callbacks/","text":"Source: vis/callbacks.py#L0 OptimizerCallback Optimizer.minimize \u3067\u4f7f\u7528\u3059\u308b\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092\u5b9a\u7fa9\u3059\u308b\u305f\u3081\u306e\u62bd\u8c61\u30af\u30e9\u30b9\u3067\u3059\u3002 OptimizerCallback.callback callback(self, i, named_losses, overall_loss, grads, wrt_value) This function will be called within optimizer.minimize . Args: i : The optimizer iteration. named_losses : List of (loss_name, loss_value) tuples. overall_loss : Overall weighted loss. grads : The gradient of input image with respect to wrt_value . wrt_value : The current wrt_value . OptimizerCallback.on_end on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization. Print \u6700\u9069\u5316\u4e2d\u306e\u5024\u3092\u30d7\u30ea\u30f3\u30c8\u3059\u308b\u305f\u3081\u306e\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3067\u3059\u3002 Print.callback callback(self, i, named_losses, overall_loss, grads, wrt_value) This function will be called within optimizer.minimize . Args: i : The optimizer iteration. named_losses : List of (loss_name, loss_value) tuples. overall_loss : Overall weighted loss. grads : The gradient of input image with respect to wrt_value . wrt_value : The current wrt_value . Print.on_end on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization. GifGenerator \u6700\u9069\u5316\u753b\u50cf\u306eGIF\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306e\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3067\u3059\u3002 GifGenerator. __init__ __init__(self, path) \u5f15\u6570: path : \u4fdd\u5b58\u3059\u308bGIF\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\u3002 GifGenerator.callback callback(self, i, named_losses, overall_loss, grads, wrt_value) This function will be called within optimizer.minimize . Args: i : The optimizer iteration. named_losses : List of (loss_name, loss_value) tuples. overall_loss : Overall weighted loss. grads : The gradient of input image with respect to wrt_value . wrt_value : The current wrt_value . GifGenerator.on_end on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization.","title":"\u30b3\u30fc\u30eb\u30d0\u30c3\u30af"},{"location":"vis.callbacks/#optimizercallback","text":"Optimizer.minimize \u3067\u4f7f\u7528\u3059\u308b\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092\u5b9a\u7fa9\u3059\u308b\u305f\u3081\u306e\u62bd\u8c61\u30af\u30e9\u30b9\u3067\u3059\u3002","title":"OptimizerCallback"},{"location":"vis.callbacks/#optimizercallbackcallback","text":"callback(self, i, named_losses, overall_loss, grads, wrt_value) This function will be called within optimizer.minimize . Args: i : The optimizer iteration. named_losses : List of (loss_name, loss_value) tuples. overall_loss : Overall weighted loss. grads : The gradient of input image with respect to wrt_value . wrt_value : The current wrt_value .","title":"OptimizerCallback.callback"},{"location":"vis.callbacks/#optimizercallbackon_end","text":"on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization.","title":"OptimizerCallback.on_end"},{"location":"vis.callbacks/#print","text":"\u6700\u9069\u5316\u4e2d\u306e\u5024\u3092\u30d7\u30ea\u30f3\u30c8\u3059\u308b\u305f\u3081\u306e\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3067\u3059\u3002","title":"Print"},{"location":"vis.callbacks/#printcallback","text":"callback(self, i, named_losses, overall_loss, grads, wrt_value) This function will be called within optimizer.minimize . Args: i : The optimizer iteration. named_losses : List of (loss_name, loss_value) tuples. overall_loss : Overall weighted loss. grads : The gradient of input image with respect to wrt_value . wrt_value : The current wrt_value .","title":"Print.callback"},{"location":"vis.callbacks/#printon_end","text":"on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization.","title":"Print.on_end"},{"location":"vis.callbacks/#gifgenerator","text":"\u6700\u9069\u5316\u753b\u50cf\u306eGIF\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306e\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3067\u3059\u3002","title":"GifGenerator"},{"location":"vis.callbacks/#gifgenerator__init__","text":"__init__(self, path) \u5f15\u6570: path : \u4fdd\u5b58\u3059\u308bGIF\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\u3002","title":"GifGenerator.__init__"},{"location":"vis.callbacks/#gifgeneratorcallback","text":"callback(self, i, named_losses, overall_loss, grads, wrt_value) This function will be called within optimizer.minimize . Args: i : The optimizer iteration. named_losses : List of (loss_name, loss_value) tuples. overall_loss : Overall weighted loss. grads : The gradient of input image with respect to wrt_value . wrt_value : The current wrt_value .","title":"GifGenerator.callback"},{"location":"vis.callbacks/#gifgeneratoron_end","text":"on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization.","title":"GifGenerator.on_end"},{"location":"vis.grad_modifiers/","text":"Source: vis/grad_modifiers.py#L0 negate negate(grads) \u52fe\u914d\u306e\u7b26\u53f7\u3092\u53cd\u8ee2\u3057\u307e\u3059\u3002\u3059\u306a\u308f\u3061-1\u3068\u306e\u7a4d\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u7b26\u53f7\u53cd\u8ee2\u3057\u305f\u52fe\u914d\u3002 absolute absolute(grads) \u52fe\u914d\u306e\u7d76\u5bfe\u5024\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u52fe\u914d\u306e\u7d76\u5bfe\u5024\u3002 invert invert(grads) \u52fe\u914d\u306e\u9006\u6570\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u52fe\u914d\u306e\u9006\u6570 relu relu(grads) \u8ca0\u306e\u52fe\u914d\u306e\u5024\u3092\u5207\u308a\u6368\u3066\u307e\u3059\u3002\u3059\u306a\u308f\u3061max(0, grads)\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u6574\u6d41\u3055\u308c\u305f\u52fe\u914d small_values small_values(grads) \u5c0f\u3055\u306a\u52fe\u914d\u306e\u5024\u307b\u3069\u5f37\u8abf\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3059\u306a\u308f\u3061absolute(invert(grads))\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u5c0f\u3055\u306a\u5024\u3092\u5f37\u8abf\u3059\u308b\u3088\u3046\u5909\u66f4\u3055\u308c\u305f\u52fe\u914d get get(identifier)","title":"\u52fe\u914d\u4fee\u98fe\u5b50"},{"location":"vis.grad_modifiers/#negate","text":"negate(grads) \u52fe\u914d\u306e\u7b26\u53f7\u3092\u53cd\u8ee2\u3057\u307e\u3059\u3002\u3059\u306a\u308f\u3061-1\u3068\u306e\u7a4d\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u7b26\u53f7\u53cd\u8ee2\u3057\u305f\u52fe\u914d\u3002","title":"negate"},{"location":"vis.grad_modifiers/#absolute","text":"absolute(grads) \u52fe\u914d\u306e\u7d76\u5bfe\u5024\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u52fe\u914d\u306e\u7d76\u5bfe\u5024\u3002","title":"absolute"},{"location":"vis.grad_modifiers/#invert","text":"invert(grads) \u52fe\u914d\u306e\u9006\u6570\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u52fe\u914d\u306e\u9006\u6570","title":"invert"},{"location":"vis.grad_modifiers/#relu","text":"relu(grads) \u8ca0\u306e\u52fe\u914d\u306e\u5024\u3092\u5207\u308a\u6368\u3066\u307e\u3059\u3002\u3059\u306a\u308f\u3061max(0, grads)\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u6574\u6d41\u3055\u308c\u305f\u52fe\u914d","title":"relu"},{"location":"vis.grad_modifiers/#small_values","text":"small_values(grads) \u5c0f\u3055\u306a\u52fe\u914d\u306e\u5024\u307b\u3069\u5f37\u8abf\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3059\u306a\u308f\u3061absolute(invert(grads))\u3002 \u5f15\u6570: grads : \u52fe\u914d\u3092\u3042\u3089\u308f\u3059Numpy array\u3002 \u623b\u308a\u5024: \u5c0f\u3055\u306a\u5024\u3092\u5f37\u8abf\u3059\u308b\u3088\u3046\u5909\u66f4\u3055\u308c\u305f\u52fe\u914d","title":"small_values"},{"location":"vis.grad_modifiers/#get","text":"get(identifier)","title":"get"},{"location":"vis.input_modifiers/","text":"Source: vis/input_modifiers.py#L0 InputModifier input modifier\u3092\u5b9a\u7fa9\u3059\u308b\u305f\u3081\u306e\u62bd\u8c61\u30af\u30e9\u30b9\u3067\u3059\u3002 Optimizer.minimize \u3067input modifier\u3092\u4f7f\u7528\u3059\u308b\u3068\u6700\u9069\u5316\u51e6\u7406\u4e2d\u3001\u6700\u9069\u5316\u3055\u308c\u308b\u5165\u529b\u5024\u306b\u5bfe\u3057\u3066\u3001\u66f4\u65b0\u524d\u5f8c\u306b\u5909\u66f4\u3092\u52a0\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 modifier.pre(seed_input) # gradient descent update to img modifier.post(seed_input) InputModifier.post post(self, inp) Implement post gradient descent update modification to the input. If post-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified post input. InputModifier.pre pre(self, inp) Implement pre gradient descent update modification to the input. If pre-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified pre input. Jitter Jitter. __init__ __init__(self, jitter=0.05) \u5165\u529b\u5024\u306e\u66f4\u65b0\u524d\u306b\u3001\u30e9\u30f3\u30c0\u30e0Jitter\u3092\u5c0e\u5165\u3059\u308b\u305f\u3081\u306einput modifier\u5b9f\u88c5\u3067\u3059\u3002 Jitter\u306f\u9bae\u660e\u306aActivationMaximization\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u3066\u3044\u307e\u3059\u3002 \u5f15\u6570: jitter : \u9069\u7528\u3055\u308c\u308bjitter\u91cf\u3092\u3042\u3089\u308f\u3059\u30b9\u30ab\u30e9\u5024\u3001\u307e\u305f\u306f\u30b7\u30fc\u30b1\u30f3\u30b9\u3002 \u30b9\u30ab\u30e9\u5024\u306e\u5834\u5408, \u540c\u3058jitter\u304c\u753b\u50cf\u306e\u5168\u3066\u306e\u6b21\u5143\u306b\u9069\u7528\u3055\u308c\u307e\u3059\u3002\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u5834\u5408\u3001 jitter \u306f\u753b\u50cf\u306e\u6b21\u5143\u3054\u3068\u306e\u5024\u3092\u542b\u3080\u3079\u304d\u3067\u3059\u3002 [0., 1.] \u306e\u9593\u306e\u5024\u306f\u753b\u50cf\u306e\u5bf8\u6cd5\u306b\u5bfe\u3059\u308b\u6bd4\u7387\u3068\u3057\u3066\u89e3\u91c8\u3055\u308c\u307e\u3059\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1a0.05\uff09 Jitter.post post(self, inp) Implement post gradient descent update modification to the input. If post-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified post input. Jitter.pre pre(self, img) Implement pre gradient descent update modification to the input. If pre-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified pre input.","title":"\u5165\u529b\u4fee\u98fe\u5b50"},{"location":"vis.input_modifiers/#inputmodifier","text":"input modifier\u3092\u5b9a\u7fa9\u3059\u308b\u305f\u3081\u306e\u62bd\u8c61\u30af\u30e9\u30b9\u3067\u3059\u3002 Optimizer.minimize \u3067input modifier\u3092\u4f7f\u7528\u3059\u308b\u3068\u6700\u9069\u5316\u51e6\u7406\u4e2d\u3001\u6700\u9069\u5316\u3055\u308c\u308b\u5165\u529b\u5024\u306b\u5bfe\u3057\u3066\u3001\u66f4\u65b0\u524d\u5f8c\u306b\u5909\u66f4\u3092\u52a0\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 modifier.pre(seed_input) # gradient descent update to img modifier.post(seed_input)","title":"InputModifier"},{"location":"vis.input_modifiers/#inputmodifierpost","text":"post(self, inp) Implement post gradient descent update modification to the input. If post-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified post input.","title":"InputModifier.post"},{"location":"vis.input_modifiers/#inputmodifierpre","text":"pre(self, inp) Implement pre gradient descent update modification to the input. If pre-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified pre input.","title":"InputModifier.pre"},{"location":"vis.input_modifiers/#jitter","text":"","title":"Jitter"},{"location":"vis.input_modifiers/#jitter__init__","text":"__init__(self, jitter=0.05) \u5165\u529b\u5024\u306e\u66f4\u65b0\u524d\u306b\u3001\u30e9\u30f3\u30c0\u30e0Jitter\u3092\u5c0e\u5165\u3059\u308b\u305f\u3081\u306einput modifier\u5b9f\u88c5\u3067\u3059\u3002 Jitter\u306f\u9bae\u660e\u306aActivationMaximization\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u3066\u3044\u307e\u3059\u3002 \u5f15\u6570: jitter : \u9069\u7528\u3055\u308c\u308bjitter\u91cf\u3092\u3042\u3089\u308f\u3059\u30b9\u30ab\u30e9\u5024\u3001\u307e\u305f\u306f\u30b7\u30fc\u30b1\u30f3\u30b9\u3002 \u30b9\u30ab\u30e9\u5024\u306e\u5834\u5408, \u540c\u3058jitter\u304c\u753b\u50cf\u306e\u5168\u3066\u306e\u6b21\u5143\u306b\u9069\u7528\u3055\u308c\u307e\u3059\u3002\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u5834\u5408\u3001 jitter \u306f\u753b\u50cf\u306e\u6b21\u5143\u3054\u3068\u306e\u5024\u3092\u542b\u3080\u3079\u304d\u3067\u3059\u3002 [0., 1.] \u306e\u9593\u306e\u5024\u306f\u753b\u50cf\u306e\u5bf8\u6cd5\u306b\u5bfe\u3059\u308b\u6bd4\u7387\u3068\u3057\u3066\u89e3\u91c8\u3055\u308c\u307e\u3059\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1a0.05\uff09","title":"Jitter.__init__"},{"location":"vis.input_modifiers/#jitterpost","text":"post(self, inp) Implement post gradient descent update modification to the input. If post-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified post input.","title":"Jitter.post"},{"location":"vis.input_modifiers/#jitterpre","text":"pre(self, img) Implement pre gradient descent update modification to the input. If pre-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified pre input.","title":"Jitter.pre"},{"location":"vis.losses/","text":"Source: vis/losses.py#L0 Loss \u6700\u5c0f\u5316\u3055\u308c\u308b\u640d\u5931\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u305f\u3081\u306e\u62bd\u8c61\u30af\u30e9\u30b9\u3067\u3059\u3002 \u640d\u5931\u95a2\u6570\u306f build_loss \u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u3066\u69cb\u7bc9\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002 name \u5c5e\u6027\u306f\u5197\u9577\u306a\u51fa\u529b\u3092\u4f34\u3046\u640d\u5931\u95a2\u6570\u3092\u8b58\u5225\u3059\u308b\u305f\u3081\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002 \u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3057\u306a\u3044\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f'Unnamed Loss'\u306b\u306a\u308a\u307e\u3059\u3002 Loss. __init__ __init__(self) \u81ea\u8eab\u3092\u521d\u671f\u5316\u3057\u307e\u3059\u3002\u6b63\u78ba\u306a\u30b7\u30b0\u30cd\u30c1\u30e3\u306b\u3064\u3044\u3066\u306f help(type(self)) \u3067\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Loss.build_loss build_loss(self) \u640d\u5931\u95a2\u6570\u5f0f\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306b\u3001\u3053\u306e\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002 \u3053\u306e\u640d\u5931\u95a2\u6570\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u8ffd\u52a0\u306e\u5f15\u6570\u306f __init__ \u7d4c\u7531\u3067\u6e21\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u7406\u60f3\u7684\u306b\u306f\u3001\u3053\u306e\u95a2\u6570\u5f0f\u306f\u5168\u3066\u306eKeras\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3068\u3001 channels_first \u3084 channels_last \u306e\u753b\u50cf\u30c7\u30fc\u30bf\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u4e92\u63db\u6027\u3092\u6301\u3064\u3079\u304d\u3067\u3059\u3002 utils.slicer \u3092\u4f7f\u3048\u3070\u3001\u30c7\u30fc\u30bf\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u6c17\u306b\u305b\u305a\u30b9\u30e9\u30a4\u30b9\u3092\u5b9a\u7fa9\u3067\u304d\u307e\u3059\u3002 \uff08 channels_first \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u305d\u308c\u5b9a\u7fa9\u3059\u308b\u3068\u3001 channels_last \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u4f7f\u3046Tensorflow\u7528\u306b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u81ea\u52d5\u7684\u306b\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u307e\u3059\u3002 \uff09 # theano slice conv_layer[:, filter_idx, ...] # TF slice conv_layer[..., filter_idx] # Backend agnostic slice conv_layer[utils.slicer[:, filter_idx, ...]] utils.get_img_shape \u306f\u3053\u308c\u3092\u7c21\u5358\u306b\u3059\u308b\u3082\u3046\u4e00\u3064\u306e\u30aa\u30d7\u30b7\u30e7\u30ca\u30eb\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u3067\u3059\u3002 \u623b\u308a\u5024: \u640d\u5931\u5f0f ActivationMaximization \u7279\u5b9a\u30ec\u30a4\u30e4\u5185\u306e\u30d5\u30a3\u30eb\u30bf\u30bb\u30c3\u30c8\u306e\u6d3b\u6027\u5316\u3092\u6700\u5927\u5316\u3059\u308b\u640d\u5931\u95a2\u6570\u3067\u3059\u3002 \u3053\u306e\u640d\u5931\u306f\u901a\u5e38\u3068\u306f\u53cd\u5bfe\u306e\u554f\u3044\u639b\u3051\u306e\u305f\u3081\u306b\u4f7f\u308f\u308c\u307e\u3059\u30fc\u3069\u306e\u3088\u3046\u306a\u7a2e\u985e\u306e\u5165\u529b\u753b\u50cf\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u78ba\u4fe1\u5ea6\u304c\u5897\u52a0\u3059\u308b\u304b\u3001\u4f8b\u3048\u3070\u72ac\u30af\u30e9\u30b9\u3067\u3002 \u3053\u308c\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5185\u5728\u5316\u3057\u3066\u3044\u308b\u3082\u306e\u304c'\u72ac'\u306e\u753b\u50cf\u7a7a\u9593\u3067\u3042\u308b\u3068\u5224\u65ad\u3059\u308b\u52a9\u3051\u306b\u306a\u308a\u307e\u3059\u3002 \u3053\u308c\u3092\u6700\u5f8c\u306e keras.layers.Dense \u30ec\u30a4\u30e4\u3067\u3001'\u72ac'\u3068'\u4eba'\u306e\u51fa\u529b\u4e21\u65b9\u3092\u6700\u5927\u5316\u3059\u308b\u5165\u529b\u753b\u50cf\u306e\u751f\u6210\u306b\u4f7f\u3046\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002 ActivationMaximization. __init__ __init__(self, layer, filter_indices) \u5f15\u6570: layer : \u6700\u5927\u5316\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u306eKeras\u30ec\u30a4\u30e4\u3002\u3053\u308c\u306fconv\u5c64\u304bdense\u5c64\u3092\u6307\u5b9a\u3067\u304d\u307e\u3059\u3002 filter_indices : \u6700\u5927\u5316\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u306eindices\u3002 keras.layers.Dense \u30ec\u30a4\u30e4\u3067\u3042\u308c\u3070\u3001 filter_idx \u306f\u51fa\u529b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3057\u3066\u89e3\u91c8\u3055\u308c\u307e\u3059\u3002 \u3082\u3057\u8cb4\u65b9\u304c\u30af\u30e9\u30b9\u51fa\u529b\u3092\u6700\u5927\u5316\u3059\u308b\u305f\u3081\u306b\u6700\u5f8c\u306e keras.layers.Dense \u30ec\u30a4\u30e4\u3092\u6700\u9069\u5316\u3059\u308b\u5834\u5408\u3001'softmax'\u3068\u306f\u5bfe\u7167\u7684\u306a\u6d3b\u6027\u5316\u3067\u3042\u308b'linear'\u306e\u65b9\u304c\u3088\u308a\u826f\u3044\u7d50\u679c\u3092\u5f97\u308b\u50be\u5411\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f softmax \u51fa\u529b\u304c\u4ed6\u306e\u30af\u30e9\u30b9\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3053\u3068\u3067\u6700\u5927\u5316\u3055\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u305f\u3081\u3067\u3059\u3002 ActivationMaximization.build_loss Returns: The loss expression.","title":"\u640d\u5931\u95a2\u6570"},{"location":"vis.losses/#loss","text":"\u6700\u5c0f\u5316\u3055\u308c\u308b\u640d\u5931\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u305f\u3081\u306e\u62bd\u8c61\u30af\u30e9\u30b9\u3067\u3059\u3002 \u640d\u5931\u95a2\u6570\u306f build_loss \u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u3066\u69cb\u7bc9\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002 name \u5c5e\u6027\u306f\u5197\u9577\u306a\u51fa\u529b\u3092\u4f34\u3046\u640d\u5931\u95a2\u6570\u3092\u8b58\u5225\u3059\u308b\u305f\u3081\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002 \u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3057\u306a\u3044\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f'Unnamed Loss'\u306b\u306a\u308a\u307e\u3059\u3002","title":"Loss"},{"location":"vis.losses/#loss__init__","text":"__init__(self) \u81ea\u8eab\u3092\u521d\u671f\u5316\u3057\u307e\u3059\u3002\u6b63\u78ba\u306a\u30b7\u30b0\u30cd\u30c1\u30e3\u306b\u3064\u3044\u3066\u306f help(type(self)) \u3067\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002","title":"Loss.__init__"},{"location":"vis.losses/#lossbuild_loss","text":"build_loss(self) \u640d\u5931\u95a2\u6570\u5f0f\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306b\u3001\u3053\u306e\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002 \u3053\u306e\u640d\u5931\u95a2\u6570\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u8ffd\u52a0\u306e\u5f15\u6570\u306f __init__ \u7d4c\u7531\u3067\u6e21\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u7406\u60f3\u7684\u306b\u306f\u3001\u3053\u306e\u95a2\u6570\u5f0f\u306f\u5168\u3066\u306eKeras\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3068\u3001 channels_first \u3084 channels_last \u306e\u753b\u50cf\u30c7\u30fc\u30bf\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u4e92\u63db\u6027\u3092\u6301\u3064\u3079\u304d\u3067\u3059\u3002 utils.slicer \u3092\u4f7f\u3048\u3070\u3001\u30c7\u30fc\u30bf\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u6c17\u306b\u305b\u305a\u30b9\u30e9\u30a4\u30b9\u3092\u5b9a\u7fa9\u3067\u304d\u307e\u3059\u3002 \uff08 channels_first \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u305d\u308c\u5b9a\u7fa9\u3059\u308b\u3068\u3001 channels_last \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u4f7f\u3046Tensorflow\u7528\u306b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u81ea\u52d5\u7684\u306b\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u307e\u3059\u3002 \uff09 # theano slice conv_layer[:, filter_idx, ...] # TF slice conv_layer[..., filter_idx] # Backend agnostic slice conv_layer[utils.slicer[:, filter_idx, ...]] utils.get_img_shape \u306f\u3053\u308c\u3092\u7c21\u5358\u306b\u3059\u308b\u3082\u3046\u4e00\u3064\u306e\u30aa\u30d7\u30b7\u30e7\u30ca\u30eb\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u3067\u3059\u3002 \u623b\u308a\u5024: \u640d\u5931\u5f0f","title":"Loss.build_loss"},{"location":"vis.losses/#activationmaximization","text":"\u7279\u5b9a\u30ec\u30a4\u30e4\u5185\u306e\u30d5\u30a3\u30eb\u30bf\u30bb\u30c3\u30c8\u306e\u6d3b\u6027\u5316\u3092\u6700\u5927\u5316\u3059\u308b\u640d\u5931\u95a2\u6570\u3067\u3059\u3002 \u3053\u306e\u640d\u5931\u306f\u901a\u5e38\u3068\u306f\u53cd\u5bfe\u306e\u554f\u3044\u639b\u3051\u306e\u305f\u3081\u306b\u4f7f\u308f\u308c\u307e\u3059\u30fc\u3069\u306e\u3088\u3046\u306a\u7a2e\u985e\u306e\u5165\u529b\u753b\u50cf\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u78ba\u4fe1\u5ea6\u304c\u5897\u52a0\u3059\u308b\u304b\u3001\u4f8b\u3048\u3070\u72ac\u30af\u30e9\u30b9\u3067\u3002 \u3053\u308c\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5185\u5728\u5316\u3057\u3066\u3044\u308b\u3082\u306e\u304c'\u72ac'\u306e\u753b\u50cf\u7a7a\u9593\u3067\u3042\u308b\u3068\u5224\u65ad\u3059\u308b\u52a9\u3051\u306b\u306a\u308a\u307e\u3059\u3002 \u3053\u308c\u3092\u6700\u5f8c\u306e keras.layers.Dense \u30ec\u30a4\u30e4\u3067\u3001'\u72ac'\u3068'\u4eba'\u306e\u51fa\u529b\u4e21\u65b9\u3092\u6700\u5927\u5316\u3059\u308b\u5165\u529b\u753b\u50cf\u306e\u751f\u6210\u306b\u4f7f\u3046\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002","title":"ActivationMaximization"},{"location":"vis.losses/#activationmaximization__init__","text":"__init__(self, layer, filter_indices) \u5f15\u6570: layer : \u6700\u5927\u5316\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u306eKeras\u30ec\u30a4\u30e4\u3002\u3053\u308c\u306fconv\u5c64\u304bdense\u5c64\u3092\u6307\u5b9a\u3067\u304d\u307e\u3059\u3002 filter_indices : \u6700\u5927\u5316\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u306eindices\u3002 keras.layers.Dense \u30ec\u30a4\u30e4\u3067\u3042\u308c\u3070\u3001 filter_idx \u306f\u51fa\u529b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3057\u3066\u89e3\u91c8\u3055\u308c\u307e\u3059\u3002 \u3082\u3057\u8cb4\u65b9\u304c\u30af\u30e9\u30b9\u51fa\u529b\u3092\u6700\u5927\u5316\u3059\u308b\u305f\u3081\u306b\u6700\u5f8c\u306e keras.layers.Dense \u30ec\u30a4\u30e4\u3092\u6700\u9069\u5316\u3059\u308b\u5834\u5408\u3001'softmax'\u3068\u306f\u5bfe\u7167\u7684\u306a\u6d3b\u6027\u5316\u3067\u3042\u308b'linear'\u306e\u65b9\u304c\u3088\u308a\u826f\u3044\u7d50\u679c\u3092\u5f97\u308b\u50be\u5411\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f softmax \u51fa\u529b\u304c\u4ed6\u306e\u30af\u30e9\u30b9\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3053\u3068\u3067\u6700\u5927\u5316\u3055\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u305f\u3081\u3067\u3059\u3002","title":"ActivationMaximization.__init__"},{"location":"vis.losses/#activationmaximizationbuild_loss","text":"Returns: The loss expression.","title":"ActivationMaximization.build_loss"},{"location":"vis.optimizer/","text":"Source: vis/optimizer.py#L0 Optimizer Optimizer. __init__ __init__(self, input_tensor, losses, input_range=(0, 255), wrt_tensor=None, norm_grads=True) Creates an optimizer that minimizes weighted loss function. \u91cd\u307f\u4ed8\u304d\u640d\u5931\u95a2\u6570\u3092\u6700\u5c0f\u5316\u3059\u308b\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u3092\u751f\u6210\u3057\u307e\u3059\u3002 \u5f15\u6570: input_tensor : \u5165\u529b\u30c6\u30f3\u30bd\u30eb\u3002\u5f62\u72b6\u306f\u3001 channels_first \u306e\u5834\u5408\u306f (samples, channels, image_dims...) \u3001 channels_last \u306e\u5834\u5408\u306f (samples, image_dims..., channels) \u3002 losses : ( Loss , weight)\u306e\u30bf\u30d7\u30eb\u306e\u30ea\u30b9\u30c8. input_range : \u5165\u529b\u5024\u306e\u7bc4\u56f2\u3092 (min, max) \u306e\u30bf\u30d7\u30eb\u3067\u6307\u5b9a\u3057\u307e\u3059\u3002 \u3053\u308c\u306f\u6700\u7d42\u7684\u306b\u6700\u9069\u5316\u3055\u308c\u305f\u5165\u529b\u3092\u3001\u6307\u5b9a\u3055\u308c\u305f\u306e\u7bc4\u56f2\u306b\u30ea\u30b9\u30b1\u30fc\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1a(0, 255)\uff09\u3002 wrt_tensor : losses \u304b\u3089\u96c6\u8a08\u3057\u305f\u640d\u5931\u3092 wtr_tensor \u306b\u95a2\u3057\u3066\u6700\u5c0f\u5316\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3053\u3068\u3092\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u306b\u6307\u793a\u3057\u307e\u3059\u3002 wrt_tensor \u306b\u306f\u30e2\u30c7\u30eb\u30b0\u30e9\u30d5\u306e\u3044\u305a\u308c\u304b\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u6307\u5b9a\u3067\u304d\u307e\u3059\u3002 \u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u306fNone\u3067\u3001\u3053\u308c\u306f\u5358\u306b\u640d\u5931\u3092 input_tensor \u306b\u95a2\u3057\u3066\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002 norm_grads : \u52fe\u914d\u3092\u6b63\u898f\u5316\u3059\u308b\u5834\u5408\u306fTrue\u3002 \u6b63\u898f\u5316\u306f\u5c0f\u3055\u3059\u304e\u308b\u307e\u305f\u306f\u5927\u304d\u3059\u304e\u308b\u52fe\u914d\u3092\u56de\u907f\u3057\u3001\u6ed1\u3089\u304b\u306a\u52fe\u914d\u964d\u4e0b\u30d7\u30ed\u30bb\u30b9\u3092\u78ba\u4fdd\u3057\u307e\u3059\u3002 \u5b9f\u969b\u306e\u52fe\u914d\u3092\u6271\u3046\u5834\u5408\u306f\uff08\u4f8b\u3048\u3070\u3001\u6ce8\u76ee\u9818\u57df\u306e\u53ef\u8996\u5316\uff09\u3001False\u3092\u30bb\u30c3\u30c8\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Optimizer.minimize minimize(self, seed_input=None, max_iter=200, input_modifiers=None, grad_modifier=None, \\ callbacks=None, verbose=True) \u5b9a\u7fa9\u3055\u308c\u305f\u640d\u5931\u306b\u95a2\u3057\u3066\u3001\u5165\u529b\u753b\u50cf\u306e\u52fe\u914d\u964d\u4e0b\u3092\u304a\u3053\u306a\u3046\u3002 \u5f15\u6570: seed_input : N\u6b21\u5143\u306enumpy array\u3002\u5f62\u72b6\u306f\u3001 channels_first \u306e\u5834\u5408\u306f (samples, channels, image_dims...) \u3001 channels_last \u306e\u5834\u5408\u306f (samples, image_dims..., channels) \u3002 None\u304c\u8a2d\u5b9a\u3055\u308c\u305f\u5834\u5408\u3001\u30e9\u30f3\u30c0\u30e0\u306a\u30ce\u30a4\u30ba\u3067\u30b7\u30fc\u30c9\u3055\u308c\u308b\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1aNone\uff09 max_iter : \u52fe\u914d\u964d\u4e0b\u3092\u7e70\u308a\u8fd4\u3059\u6700\u5927\u6570\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1a\uff12\uff10\uff10\uff09 input_modifiers : \u6700\u9069\u5316\u30d7\u30ed\u30bb\u30b9\u4e2d\u3001\u6700\u9069\u5316\u3055\u308c\u308b\u5165\u529b\u3092\u3069\u306e\u3088\u3046\u306b\u5909\u66f4\u3059\u308b\u304b\u3001 pre \u3068 post \u3067\u6307\u5b9a\u3055\u308c\u305f InputModifier \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u30ea\u30b9\u30c8\u3002 pre \u306f\u30ea\u30b9\u30c8\u306e\u4e26\u3073\u9806\u306b\u9069\u7528\u3055\u308c\u3001 post \u306f\u9006\u9806\u306b\u9069\u7528\u3055\u308c\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001 input_modifiers = [f, g] \u306f pre_input = g(f(inp)) \u3068 post_input = f(g(inp)) \u3092\u610f\u5473\u3057\u307e\u3059\u3002 grad_modifier : \u4f7f\u7528\u3059\u308bgradient modifier\u306b\u3064\u3044\u3066\u306f grad_modifiers \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u3000\u3000\u4f55\u3082\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u3001\u52fe\u914d\u306f\u5909\u66f4\u3055\u308c\u307e\u305b\u3093\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1aNone\uff09 callbacks : \u30c8\u30ea\u30ac\u30fc\u3059\u308b OptimizerCallback \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u30ea\u30b9\u30c8\u3002 verbose : \u5404\u52fe\u914d\u964d\u4e0b\u53cd\u5fa9\u306e\u6700\u5f8c\u306b\u3001\u5404\u640d\u5931\u306e\u30ed\u30b0\u3092\u51fa\u529b\u3059\u308b\u3002\u640d\u5931\u4fc2\u6570\u3092\u898b\u7a4d\u3082\u308b\u306e\u306b\u3068\u3066\u3082\u6709\u7528\u3067\u3059\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1aTrue\uff09 \u623b\u308a\u5024: \u52fe\u914d\u964d\u4e0b\u5f8c\u306e (optimized input, grads with respect to wrt, wrt_value) \u306e\u30bf\u30d7\u30eb","title":"\u6700\u9069\u5316"},{"location":"vis.optimizer/#optimizer","text":"","title":"Optimizer"},{"location":"vis.optimizer/#optimizer__init__","text":"__init__(self, input_tensor, losses, input_range=(0, 255), wrt_tensor=None, norm_grads=True) Creates an optimizer that minimizes weighted loss function. \u91cd\u307f\u4ed8\u304d\u640d\u5931\u95a2\u6570\u3092\u6700\u5c0f\u5316\u3059\u308b\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u3092\u751f\u6210\u3057\u307e\u3059\u3002 \u5f15\u6570: input_tensor : \u5165\u529b\u30c6\u30f3\u30bd\u30eb\u3002\u5f62\u72b6\u306f\u3001 channels_first \u306e\u5834\u5408\u306f (samples, channels, image_dims...) \u3001 channels_last \u306e\u5834\u5408\u306f (samples, image_dims..., channels) \u3002 losses : ( Loss , weight)\u306e\u30bf\u30d7\u30eb\u306e\u30ea\u30b9\u30c8. input_range : \u5165\u529b\u5024\u306e\u7bc4\u56f2\u3092 (min, max) \u306e\u30bf\u30d7\u30eb\u3067\u6307\u5b9a\u3057\u307e\u3059\u3002 \u3053\u308c\u306f\u6700\u7d42\u7684\u306b\u6700\u9069\u5316\u3055\u308c\u305f\u5165\u529b\u3092\u3001\u6307\u5b9a\u3055\u308c\u305f\u306e\u7bc4\u56f2\u306b\u30ea\u30b9\u30b1\u30fc\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1a(0, 255)\uff09\u3002 wrt_tensor : losses \u304b\u3089\u96c6\u8a08\u3057\u305f\u640d\u5931\u3092 wtr_tensor \u306b\u95a2\u3057\u3066\u6700\u5c0f\u5316\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3053\u3068\u3092\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u306b\u6307\u793a\u3057\u307e\u3059\u3002 wrt_tensor \u306b\u306f\u30e2\u30c7\u30eb\u30b0\u30e9\u30d5\u306e\u3044\u305a\u308c\u304b\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u6307\u5b9a\u3067\u304d\u307e\u3059\u3002 \u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u306fNone\u3067\u3001\u3053\u308c\u306f\u5358\u306b\u640d\u5931\u3092 input_tensor \u306b\u95a2\u3057\u3066\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002 norm_grads : \u52fe\u914d\u3092\u6b63\u898f\u5316\u3059\u308b\u5834\u5408\u306fTrue\u3002 \u6b63\u898f\u5316\u306f\u5c0f\u3055\u3059\u304e\u308b\u307e\u305f\u306f\u5927\u304d\u3059\u304e\u308b\u52fe\u914d\u3092\u56de\u907f\u3057\u3001\u6ed1\u3089\u304b\u306a\u52fe\u914d\u964d\u4e0b\u30d7\u30ed\u30bb\u30b9\u3092\u78ba\u4fdd\u3057\u307e\u3059\u3002 \u5b9f\u969b\u306e\u52fe\u914d\u3092\u6271\u3046\u5834\u5408\u306f\uff08\u4f8b\u3048\u3070\u3001\u6ce8\u76ee\u9818\u57df\u306e\u53ef\u8996\u5316\uff09\u3001False\u3092\u30bb\u30c3\u30c8\u3057\u3066\u304f\u3060\u3055\u3044\u3002","title":"Optimizer.__init__"},{"location":"vis.optimizer/#optimizerminimize","text":"minimize(self, seed_input=None, max_iter=200, input_modifiers=None, grad_modifier=None, \\ callbacks=None, verbose=True) \u5b9a\u7fa9\u3055\u308c\u305f\u640d\u5931\u306b\u95a2\u3057\u3066\u3001\u5165\u529b\u753b\u50cf\u306e\u52fe\u914d\u964d\u4e0b\u3092\u304a\u3053\u306a\u3046\u3002 \u5f15\u6570: seed_input : N\u6b21\u5143\u306enumpy array\u3002\u5f62\u72b6\u306f\u3001 channels_first \u306e\u5834\u5408\u306f (samples, channels, image_dims...) \u3001 channels_last \u306e\u5834\u5408\u306f (samples, image_dims..., channels) \u3002 None\u304c\u8a2d\u5b9a\u3055\u308c\u305f\u5834\u5408\u3001\u30e9\u30f3\u30c0\u30e0\u306a\u30ce\u30a4\u30ba\u3067\u30b7\u30fc\u30c9\u3055\u308c\u308b\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1aNone\uff09 max_iter : \u52fe\u914d\u964d\u4e0b\u3092\u7e70\u308a\u8fd4\u3059\u6700\u5927\u6570\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1a\uff12\uff10\uff10\uff09 input_modifiers : \u6700\u9069\u5316\u30d7\u30ed\u30bb\u30b9\u4e2d\u3001\u6700\u9069\u5316\u3055\u308c\u308b\u5165\u529b\u3092\u3069\u306e\u3088\u3046\u306b\u5909\u66f4\u3059\u308b\u304b\u3001 pre \u3068 post \u3067\u6307\u5b9a\u3055\u308c\u305f InputModifier \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u30ea\u30b9\u30c8\u3002 pre \u306f\u30ea\u30b9\u30c8\u306e\u4e26\u3073\u9806\u306b\u9069\u7528\u3055\u308c\u3001 post \u306f\u9006\u9806\u306b\u9069\u7528\u3055\u308c\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001 input_modifiers = [f, g] \u306f pre_input = g(f(inp)) \u3068 post_input = f(g(inp)) \u3092\u610f\u5473\u3057\u307e\u3059\u3002 grad_modifier : \u4f7f\u7528\u3059\u308bgradient modifier\u306b\u3064\u3044\u3066\u306f grad_modifiers \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u3000\u3000\u4f55\u3082\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u3001\u52fe\u914d\u306f\u5909\u66f4\u3055\u308c\u307e\u305b\u3093\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1aNone\uff09 callbacks : \u30c8\u30ea\u30ac\u30fc\u3059\u308b OptimizerCallback \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u30ea\u30b9\u30c8\u3002 verbose : \u5404\u52fe\u914d\u964d\u4e0b\u53cd\u5fa9\u306e\u6700\u5f8c\u306b\u3001\u5404\u640d\u5931\u306e\u30ed\u30b0\u3092\u51fa\u529b\u3059\u308b\u3002\u640d\u5931\u4fc2\u6570\u3092\u898b\u7a4d\u3082\u308b\u306e\u306b\u3068\u3066\u3082\u6709\u7528\u3067\u3059\u3002\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\uff1aTrue\uff09 \u623b\u308a\u5024: \u52fe\u914d\u964d\u4e0b\u5f8c\u306e (optimized input, grads with respect to wrt, wrt_value) \u306e\u30bf\u30d7\u30eb","title":"Optimizer.minimize"},{"location":"vis.regularizers/","text":"Source: vis/regularizers.py#L0 normalize normalize(input_tensor, output_tensor) Normalizes the output_tensor with respect to input_tensor dimensions. This makes regularizer weight factor more or less uniform across various input image dimensions. Args: input_tensor : An tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . output_tensor : The tensor to normalize. Returns: The normalized tensor. TotalVariation Abstract class for defining the loss function to be minimized. The loss function should be built by defining build_loss function. The attribute name should be defined to identify loss function with verbose outputs. Defaults to 'Unnamed Loss' if not overridden. TotalVariation. __init__ __init__(self, img_input, beta=2.0) Total variation regularizer encourages blobbier and coherent image structures, akin to natural images. See section 3.2.2 in Visualizing deep convolutional neural networks using natural pre-images for details. Args: img_input : An image tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last`. beta : Smaller values of beta give sharper but 'spikier' images. Values \\in [1.5, 3.0] are recommended as a reasonable compromise. (Default value = 2.) TotalVariation.build_loss build_loss(self) Implements the N-dim version of function TV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} + \\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}} to return total variation for all images in the batch. LPNorm Abstract class for defining the loss function to be minimized. The loss function should be built by defining build_loss function. The attribute name should be defined to identify loss function with verbose outputs. Defaults to 'Unnamed Loss' if not overridden. LPNorm. __init__ __init__(self, img_input, p=6.0) Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded. i.e., prevents pixels from taking on very large values. Args: img_input : 4D image input tensor to the model of shape: (samples, channels, rows, cols) if data_format='channels_first' or (samples, rows, cols, channels) if data_format='channels_last'. p : The pth norm to use. If p = float('inf'), infinity-norm will be used. LPNorm.build_loss build_loss(self) Implement this function to build the loss function expression. Any additional arguments required to build this loss function may be passed in via __init__ . Ideally, the function expression must be compatible with all keras backends and channels_first or channels_last image_data_format(s). utils.slicer can be used to define data format agnostic slices. (just define it in channels_first format, it will automatically shuffle indices for tensorflow which uses channels_last format). # theano slice conv_layer[:, filter_idx, ...] # TF slice conv_layer[..., filter_idx] # Backend agnostic slice conv_layer[utils.slicer[:, filter_idx, ...]] utils.get_img_shape is another optional utility that make this easier. Returns: The loss expression.","title":"\u6b63\u5247\u5316"},{"location":"vis.regularizers/#normalize","text":"normalize(input_tensor, output_tensor) Normalizes the output_tensor with respect to input_tensor dimensions. This makes regularizer weight factor more or less uniform across various input image dimensions. Args: input_tensor : An tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . output_tensor : The tensor to normalize. Returns: The normalized tensor.","title":"normalize"},{"location":"vis.regularizers/#totalvariation","text":"Abstract class for defining the loss function to be minimized. The loss function should be built by defining build_loss function. The attribute name should be defined to identify loss function with verbose outputs. Defaults to 'Unnamed Loss' if not overridden.","title":"TotalVariation"},{"location":"vis.regularizers/#totalvariation__init__","text":"__init__(self, img_input, beta=2.0) Total variation regularizer encourages blobbier and coherent image structures, akin to natural images. See section 3.2.2 in Visualizing deep convolutional neural networks using natural pre-images for details. Args: img_input : An image tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last`. beta : Smaller values of beta give sharper but 'spikier' images. Values \\in [1.5, 3.0] are recommended as a reasonable compromise. (Default value = 2.)","title":"TotalVariation.__init__"},{"location":"vis.regularizers/#totalvariationbuild_loss","text":"build_loss(self) Implements the N-dim version of function TV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} + \\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}} to return total variation for all images in the batch.","title":"TotalVariation.build_loss"},{"location":"vis.regularizers/#lpnorm","text":"Abstract class for defining the loss function to be minimized. The loss function should be built by defining build_loss function. The attribute name should be defined to identify loss function with verbose outputs. Defaults to 'Unnamed Loss' if not overridden.","title":"LPNorm"},{"location":"vis.regularizers/#lpnorm__init__","text":"__init__(self, img_input, p=6.0) Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded. i.e., prevents pixels from taking on very large values. Args: img_input : 4D image input tensor to the model of shape: (samples, channels, rows, cols) if data_format='channels_first' or (samples, rows, cols, channels) if data_format='channels_last'. p : The pth norm to use. If p = float('inf'), infinity-norm will be used.","title":"LPNorm.__init__"},{"location":"vis.regularizers/#lpnormbuild_loss","text":"build_loss(self) Implement this function to build the loss function expression. Any additional arguments required to build this loss function may be passed in via __init__ . Ideally, the function expression must be compatible with all keras backends and channels_first or channels_last image_data_format(s). utils.slicer can be used to define data format agnostic slices. (just define it in channels_first format, it will automatically shuffle indices for tensorflow which uses channels_last format). # theano slice conv_layer[:, filter_idx, ...] # TF slice conv_layer[..., filter_idx] # Backend agnostic slice conv_layer[utils.slicer[:, filter_idx, ...]] utils.get_img_shape is another optional utility that make this easier. Returns: The loss expression.","title":"LPNorm.build_loss"},{"location":"vis.utils.utils/","text":"Source: vis/utils/utils.py#L0 Global Variables slicer reverse_enumerate reverse_enumerate(iterable) Enumerate over an iterable in reverse order while retaining proper indexes, without creating any copies. listify listify(value) Ensures that the value is a list. If it is not a list, it creates a new list with value as an item. add_defaults_to_kwargs add_defaults_to_kwargs(defaults, **kwargs) Updates kwargs with dict of defaults Args: defaults : A dictionary of keys and values **kwargs: The kwargs to update. Returns: The updated kwargs. get_identifier get_identifier(identifier, module_globals, module_name) Helper utility to retrieve the callable function associated with a string identifier. Args: identifier : The identifier. Could be a string or function. module_globals : The global objects of the module. module_name : The module name Returns: The callable associated with the identifier. apply_modifications apply_modifications(model, custom_objects=None) Applies modifications to the model layers to create a new Graph. For example, simply changing model.layers[idx].activation = new activation does not change the graph. The entire graph needs to be updated with modified inbound and outbound tensors because of change in layer building function. Args: model : The keras.models.Model instance. Returns: The modified model with changes applied. Does not mutate the original model . random_array random_array(shape, mean=128.0, std=20.0) Creates a uniformly distributed random array with the given mean and std . Args: shape : The desired shape mean : The desired mean (Default value = 128) std : The desired std (Default value = 20) Returns: Random numpy array of given shape uniformly distributed with desired mean and std . find_layer_idx find_layer_idx(model, layer_name) Looks up the layer index corresponding to layer_name from model . Args: model : The keras.models.Model instance. layer_name : The name of the layer to lookup. Returns: The layer index if found. Raises an exception otherwise. deprocess_input deprocess_input(input_array, input_range=(0, 255)) Utility function to scale the input_array to input_range throwing away high frequency artifacts. Args: input_array : An N-dim numpy array. input_range : Specifies the input range as a (min, max) tuple to rescale the input_array . Returns: The rescaled input_array . stitch_images stitch_images(images, margin=5, cols=5) Utility function to stitch images together with a margin . Args: images : The array of 2D images to stitch. margin : The black border margin size between images (Default value = 5) cols : Max number of image cols. New row is created when number of images exceed the column size. (Default value = 5) Returns: A single numpy image array comprising of input images. get_img_shape get_img_shape(img) Returns image shape in a backend agnostic manner. Args: img : An image tensor of shape: (channels, image_dims...) if data_format='channels_first' or (image_dims..., channels) if data_format='channels_last'. Returns: Tuple containing image shape information in (samples, channels, image_dims...) order. load_img load_img(path, grayscale=False, target_size=None) Utility function to load an image from disk. Args: path : The image file path. grayscale : True to convert to grayscale image (Default value = False) target_size : (w, h) to resize. (Default value = None) Returns: The loaded numpy image. lookup_imagenet_labels lookup_imagenet_labels(indices) Utility function to return the image net label for the final dense layer output index. Args: indices : Could be a single value or an array of indices whose labels should be looked up. Returns: Image net label corresponding to the image category. draw_text draw_text(img, text, position=(10, 10), font=\"FreeSans.ttf\", font_size=14, color=(0, 0, 0)) Draws text over the image. Requires PIL. Args: img : The image to use. text : The text string to overlay. position : The text (x, y) position. (Default value = (10, 10)) font : The ttf or open type font to use. (Default value = 'FreeSans.ttf') font_size : The text font size. (Default value = 12) color : The (r, g, b) values for text color. (Default value = (0, 0, 0)) Returns: Image overlayed with text. bgr2rgb bgr2rgb(img) Converts an RGB image to BGR and vice versa Args: img : Numpy array in RGB or BGR format Returns: The converted image format normalize normalize(array, min_value=0.0, max_value=1.0) Normalizes the numpy array to (min_value, max_value) Args: array : The numpy array min_value : The min value in normalized array (Default value = 0) max_value : The max value in normalized array (Default value = 1) Returns: The array normalized to range between (min_value, max_value)","title":"\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3"},{"location":"vis.utils.utils/#global-variables","text":"slicer","title":"Global Variables"},{"location":"vis.utils.utils/#reverse_enumerate","text":"reverse_enumerate(iterable) Enumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.","title":"reverse_enumerate"},{"location":"vis.utils.utils/#listify","text":"listify(value) Ensures that the value is a list. If it is not a list, it creates a new list with value as an item.","title":"listify"},{"location":"vis.utils.utils/#add_defaults_to_kwargs","text":"add_defaults_to_kwargs(defaults, **kwargs) Updates kwargs with dict of defaults Args: defaults : A dictionary of keys and values **kwargs: The kwargs to update. Returns: The updated kwargs.","title":"add_defaults_to_kwargs"},{"location":"vis.utils.utils/#get_identifier","text":"get_identifier(identifier, module_globals, module_name) Helper utility to retrieve the callable function associated with a string identifier. Args: identifier : The identifier. Could be a string or function. module_globals : The global objects of the module. module_name : The module name Returns: The callable associated with the identifier.","title":"get_identifier"},{"location":"vis.utils.utils/#apply_modifications","text":"apply_modifications(model, custom_objects=None) Applies modifications to the model layers to create a new Graph. For example, simply changing model.layers[idx].activation = new activation does not change the graph. The entire graph needs to be updated with modified inbound and outbound tensors because of change in layer building function. Args: model : The keras.models.Model instance. Returns: The modified model with changes applied. Does not mutate the original model .","title":"apply_modifications"},{"location":"vis.utils.utils/#random_array","text":"random_array(shape, mean=128.0, std=20.0) Creates a uniformly distributed random array with the given mean and std . Args: shape : The desired shape mean : The desired mean (Default value = 128) std : The desired std (Default value = 20) Returns: Random numpy array of given shape uniformly distributed with desired mean and std .","title":"random_array"},{"location":"vis.utils.utils/#find_layer_idx","text":"find_layer_idx(model, layer_name) Looks up the layer index corresponding to layer_name from model . Args: model : The keras.models.Model instance. layer_name : The name of the layer to lookup. Returns: The layer index if found. Raises an exception otherwise.","title":"find_layer_idx"},{"location":"vis.utils.utils/#deprocess_input","text":"deprocess_input(input_array, input_range=(0, 255)) Utility function to scale the input_array to input_range throwing away high frequency artifacts. Args: input_array : An N-dim numpy array. input_range : Specifies the input range as a (min, max) tuple to rescale the input_array . Returns: The rescaled input_array .","title":"deprocess_input"},{"location":"vis.utils.utils/#stitch_images","text":"stitch_images(images, margin=5, cols=5) Utility function to stitch images together with a margin . Args: images : The array of 2D images to stitch. margin : The black border margin size between images (Default value = 5) cols : Max number of image cols. New row is created when number of images exceed the column size. (Default value = 5) Returns: A single numpy image array comprising of input images.","title":"stitch_images"},{"location":"vis.utils.utils/#get_img_shape","text":"get_img_shape(img) Returns image shape in a backend agnostic manner. Args: img : An image tensor of shape: (channels, image_dims...) if data_format='channels_first' or (image_dims..., channels) if data_format='channels_last'. Returns: Tuple containing image shape information in (samples, channels, image_dims...) order.","title":"get_img_shape"},{"location":"vis.utils.utils/#load_img","text":"load_img(path, grayscale=False, target_size=None) Utility function to load an image from disk. Args: path : The image file path. grayscale : True to convert to grayscale image (Default value = False) target_size : (w, h) to resize. (Default value = None) Returns: The loaded numpy image.","title":"load_img"},{"location":"vis.utils.utils/#lookup_imagenet_labels","text":"lookup_imagenet_labels(indices) Utility function to return the image net label for the final dense layer output index. Args: indices : Could be a single value or an array of indices whose labels should be looked up. Returns: Image net label corresponding to the image category.","title":"lookup_imagenet_labels"},{"location":"vis.utils.utils/#draw_text","text":"draw_text(img, text, position=(10, 10), font=\"FreeSans.ttf\", font_size=14, color=(0, 0, 0)) Draws text over the image. Requires PIL. Args: img : The image to use. text : The text string to overlay. position : The text (x, y) position. (Default value = (10, 10)) font : The ttf or open type font to use. (Default value = 'FreeSans.ttf') font_size : The text font size. (Default value = 12) color : The (r, g, b) values for text color. (Default value = (0, 0, 0)) Returns: Image overlayed with text.","title":"draw_text"},{"location":"vis.utils.utils/#bgr2rgb","text":"bgr2rgb(img) Converts an RGB image to BGR and vice versa Args: img : Numpy array in RGB or BGR format Returns: The converted image format","title":"bgr2rgb"},{"location":"vis.utils.utils/#normalize","text":"normalize(array, min_value=0.0, max_value=1.0) Normalizes the numpy array to (min_value, max_value) Args: array : The numpy array min_value : The min value in normalized array (Default value = 0) max_value : The max value in normalized array (Default value = 1) Returns: The array normalized to range between (min_value, max_value)","title":"normalize"},{"location":"vis.visualization/","text":"Source: vis/visualization/ init .py#L0 visualize_activation_with_losses visualize_activation_with_losses(input_tensor, losses, wrt_tensor=None, seed_input=None, \\ input_range=(0, 255), **optimizer_params) Generates the input_tensor that minimizes the weighted losses . This function is intended for advanced use cases where a custom loss is desired. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) losses : List of ( Loss , weight) tuples. seed_input : Seeds the optimization with a starting image. Initialized with a random value when set to None. (Default value = None) input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) optimizer_params : The **kwargs for optimizer params . Will default to reasonable values when required keys are not found. Returns: The model input that minimizes the weighted losses . get_num_filters get_num_filters(layer) Determines the number of filters within the given layer . Args: layer : The keras layer to use. Returns: Total number of filters within layer . For keras.layers.Dense layer, this is the total number of outputs. overlay overlay(array1, array2, alpha=0.5) Overlays array1 onto array2 with alpha blending. Args: array1 : The first numpy array. array2 : The second numpy array. alpha : The alpha value of array1 as overlayed onto array2 . This value needs to be between [0, 1], with 0 being array2 only to 1 being array1 only (Default value = 0.5). Returns: The array1 , overlayed with array2 using alpha blending. visualize_saliency_with_losses visualize_saliency_with_losses(input_tensor, losses, seed_input, wrt_tensor=None, \\ grad_modifier=\"absolute\") Generates an attention heatmap over the seed_input by using positive gradients of input_tensor with respect to weighted losses . This function is intended for advanced use cases where a custom loss is desired. For common use cases, refer to visualize_class_saliency or visualize_regression_saliency . For a full description of saliency, see the paper: [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps] (https://arxiv.org/pdf/1312.6034v2.pdf) Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. seed_input : The model input for which activation map needs to be visualized. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) grad_modifier : gradient modifier to use. See grad_modifiers . By default absolute value of gradients are used. To visualize positive or negative gradients, use relu and negate respectively. (Default value = 'absolute') Returns: The normalized gradients of seed_input with respect to weighted losses . visualize_activation visualize_activation(model, layer_idx, filter_indices=None, wrt_tensor=None, seed_input=None, \\ input_range=(0, 255), backprop_modifier=None, grad_modifier=None, act_max_weight=1, \\ lp_norm_weight=10, tv_weight=10, **optimizer_params) Generates the model input that maximizes the output of all filter_indices in the given layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) seed_input : Seeds the optimization with a starting input. Initialized with a random value when set to None. (Default value = None) input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) act_max_weight : The weight param for ActivationMaximization loss. Not used if 0 or None. (Default value = 1) lp_norm_weight : The weight param for LPNorm regularization loss. Not used if 0 or None. (Default value = 10) tv_weight : The weight param for TotalVariation regularization loss. Not used if 0 or None. (Default value = 10) optimizer_params : The **kwargs for optimizer params . Will default to reasonable values when required keys are not found. Example: If you wanted to visualize the input image that would maximize the output index 22, say on final keras.layers.Dense layer, then, filter_indices = [22] , layer_idx = dense_layer_idx . If filter_indices = [22, 23] , then it should generate an input image that shows features of both classes. Returns: The model input that maximizes the output of filter_indices in the given layer_idx . visualize_saliency visualize_saliency(model, layer_idx, filter_indices, seed_input, wrt_tensor=None, \\ backprop_modifier=None, grad_modifier=\"absolute\") Generates an attention heatmap over the seed_input for maximizing filter_indices output in the given layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. seed_input : The model input for which activation map needs to be visualized. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . By default absolute value of gradients are used. To visualize positive or negative gradients, use relu and negate respectively. (Default value = 'absolute') Example: If you wanted to visualize attention over 'bird' category, say output index 22 on the final keras.layers.Dense layer, then, filter_indices = [22] , layer = dense_layer . One could also set filter indices to more than one value. For example, filter_indices = [22, 23] should (hopefully) show attention map that corresponds to both 22, 23 output categories. Returns: The heatmap image indicating the seed_input regions whose change would most contribute towards maximizing the output of filter_indices . visualize_cam_with_losses visualize_cam_with_losses(input_tensor, losses, seed_input, penultimate_layer, grad_modifier=None) Generates a gradient based class activation map (CAM) by using positive gradients of input_tensor with respect to weighted losses . For details on grad-CAM, see the paper: [Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization] (https://arxiv.org/pdf/1610.02391v1.pdf). Unlike class activation mapping , which requires minor changes to network architecture in some instances, grad-CAM has a more general applicability. Compared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights cat regions and not the 'dog' region and vice-versa. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. seed_input : The model input for which activation map needs to be visualized. penultimate_layer : The pre-layer to layer_idx whose feature maps should be used to compute gradients with respect to filter output. grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) Returns: The normalized gradients of seed_input with respect to weighted losses . visualize_cam visualize_cam(model, layer_idx, filter_indices, seed_input, penultimate_layer_idx=None, \\ backprop_modifier=None, grad_modifier=None) Generates a gradient based class activation map (grad-CAM) that maximizes the outputs of filter_indices in layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. seed_input : The input image for which activation map needs to be visualized. penultimate_layer_idx : The pre-layer to layer_idx whose feature maps should be used to compute gradients wrt filter output. If not provided, it is set to the nearest penultimate Conv or Pooling layer. backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) Example: If you wanted to visualize attention over 'bird' category, say output index 22 on the final keras.layers.Dense layer, then, filter_indices = [22] , layer = dense_layer . One could also set filter indices to more than one value. For example, filter_indices = [22, 23] should (hopefully) show attention map that corresponds to both 22, 23 output categories. Returns: The heatmap image indicating the input regions whose change would most contribute towards maximizing the output of filter_indices .","title":"\u53ef\u8996\u5316"},{"location":"vis.visualization/#visualize_activation_with_losses","text":"visualize_activation_with_losses(input_tensor, losses, wrt_tensor=None, seed_input=None, \\ input_range=(0, 255), **optimizer_params) Generates the input_tensor that minimizes the weighted losses . This function is intended for advanced use cases where a custom loss is desired. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) losses : List of ( Loss , weight) tuples. seed_input : Seeds the optimization with a starting image. Initialized with a random value when set to None. (Default value = None) input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) optimizer_params : The **kwargs for optimizer params . Will default to reasonable values when required keys are not found. Returns: The model input that minimizes the weighted losses .","title":"visualize_activation_with_losses"},{"location":"vis.visualization/#get_num_filters","text":"get_num_filters(layer) Determines the number of filters within the given layer . Args: layer : The keras layer to use. Returns: Total number of filters within layer . For keras.layers.Dense layer, this is the total number of outputs.","title":"get_num_filters"},{"location":"vis.visualization/#overlay","text":"overlay(array1, array2, alpha=0.5) Overlays array1 onto array2 with alpha blending. Args: array1 : The first numpy array. array2 : The second numpy array. alpha : The alpha value of array1 as overlayed onto array2 . This value needs to be between [0, 1], with 0 being array2 only to 1 being array1 only (Default value = 0.5). Returns: The array1 , overlayed with array2 using alpha blending.","title":"overlay"},{"location":"vis.visualization/#visualize_saliency_with_losses","text":"visualize_saliency_with_losses(input_tensor, losses, seed_input, wrt_tensor=None, \\ grad_modifier=\"absolute\") Generates an attention heatmap over the seed_input by using positive gradients of input_tensor with respect to weighted losses . This function is intended for advanced use cases where a custom loss is desired. For common use cases, refer to visualize_class_saliency or visualize_regression_saliency . For a full description of saliency, see the paper: [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps] (https://arxiv.org/pdf/1312.6034v2.pdf) Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. seed_input : The model input for which activation map needs to be visualized. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) grad_modifier : gradient modifier to use. See grad_modifiers . By default absolute value of gradients are used. To visualize positive or negative gradients, use relu and negate respectively. (Default value = 'absolute') Returns: The normalized gradients of seed_input with respect to weighted losses .","title":"visualize_saliency_with_losses"},{"location":"vis.visualization/#visualize_activation","text":"visualize_activation(model, layer_idx, filter_indices=None, wrt_tensor=None, seed_input=None, \\ input_range=(0, 255), backprop_modifier=None, grad_modifier=None, act_max_weight=1, \\ lp_norm_weight=10, tv_weight=10, **optimizer_params) Generates the model input that maximizes the output of all filter_indices in the given layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) seed_input : Seeds the optimization with a starting input. Initialized with a random value when set to None. (Default value = None) input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) act_max_weight : The weight param for ActivationMaximization loss. Not used if 0 or None. (Default value = 1) lp_norm_weight : The weight param for LPNorm regularization loss. Not used if 0 or None. (Default value = 10) tv_weight : The weight param for TotalVariation regularization loss. Not used if 0 or None. (Default value = 10) optimizer_params : The **kwargs for optimizer params . Will default to reasonable values when required keys are not found. Example: If you wanted to visualize the input image that would maximize the output index 22, say on final keras.layers.Dense layer, then, filter_indices = [22] , layer_idx = dense_layer_idx . If filter_indices = [22, 23] , then it should generate an input image that shows features of both classes. Returns: The model input that maximizes the output of filter_indices in the given layer_idx .","title":"visualize_activation"},{"location":"vis.visualization/#visualize_saliency","text":"visualize_saliency(model, layer_idx, filter_indices, seed_input, wrt_tensor=None, \\ backprop_modifier=None, grad_modifier=\"absolute\") Generates an attention heatmap over the seed_input for maximizing filter_indices output in the given layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. seed_input : The model input for which activation map needs to be visualized. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . By default absolute value of gradients are used. To visualize positive or negative gradients, use relu and negate respectively. (Default value = 'absolute') Example: If you wanted to visualize attention over 'bird' category, say output index 22 on the final keras.layers.Dense layer, then, filter_indices = [22] , layer = dense_layer . One could also set filter indices to more than one value. For example, filter_indices = [22, 23] should (hopefully) show attention map that corresponds to both 22, 23 output categories. Returns: The heatmap image indicating the seed_input regions whose change would most contribute towards maximizing the output of filter_indices .","title":"visualize_saliency"},{"location":"vis.visualization/#visualize_cam_with_losses","text":"visualize_cam_with_losses(input_tensor, losses, seed_input, penultimate_layer, grad_modifier=None) Generates a gradient based class activation map (CAM) by using positive gradients of input_tensor with respect to weighted losses . For details on grad-CAM, see the paper: [Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization] (https://arxiv.org/pdf/1610.02391v1.pdf). Unlike class activation mapping , which requires minor changes to network architecture in some instances, grad-CAM has a more general applicability. Compared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights cat regions and not the 'dog' region and vice-versa. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. seed_input : The model input for which activation map needs to be visualized. penultimate_layer : The pre-layer to layer_idx whose feature maps should be used to compute gradients with respect to filter output. grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) Returns: The normalized gradients of seed_input with respect to weighted losses .","title":"visualize_cam_with_losses"},{"location":"vis.visualization/#visualize_cam","text":"visualize_cam(model, layer_idx, filter_indices, seed_input, penultimate_layer_idx=None, \\ backprop_modifier=None, grad_modifier=None) Generates a gradient based class activation map (grad-CAM) that maximizes the outputs of filter_indices in layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. seed_input : The input image for which activation map needs to be visualized. penultimate_layer_idx : The pre-layer to layer_idx whose feature maps should be used to compute gradients wrt filter output. If not provided, it is set to the nearest penultimate Conv or Pooling layer. backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) Example: If you wanted to visualize attention over 'bird' category, say output index 22 on the final keras.layers.Dense layer, then, filter_indices = [22] , layer = dense_layer . One could also set filter indices to more than one value. For example, filter_indices = [22, 23] should (hopefully) show attention map that corresponds to both 22, 23 output categories. Returns: The heatmap image indicating the input regions whose change would most contribute towards maximizing the output of filter_indices .","title":"visualize_cam"},{"location":"visualizations/activation_maximization/","text":"Activation Maximization\u3068\u306f CNN\u3067\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306b\u306f\u3001\u5165\u529b\u753b\u50cf\u306b\u3088\u304f\u4f3c\u305f\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d1\u30bf\u30fc\u30f3\u3092\u5165\u529b\u3059\u308b\u3068\u3001\u305d\u306e\u51fa\u529b\u3092\u6700\u5927\u5316\u3059\u308b\u3044\u304f\u3064\u304b\u306e\u5b66\u7fd2\u6e08\u307f \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30de\u30c3\u30c1\u30f3\u30b0 \u30d5\u30a3\u30eb\u30bf\u304c\u3042\u308a\u307e\u3059\u3002 \u6700\u521d\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306f\u7c21\u5358\u306b\u89e3\u91c8\u3067\u3001\u5358\u306b\u91cd\u307f\u3092\u753b\u50cf\u3068\u3057\u3066\u53ef\u8996\u5316\u3059\u308b\u3060\u3051\u3067\u3059\u3002 \u7573\u307f\u8fbc\u307f\u5c64\u304c\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u898b\u308b\u305f\u3081\u306b\u306f\u3001\u5165\u529b\u30d4\u30af\u30bb\u30eb\u306b\u30d5\u30a3\u30eb\u30bf\u3092\u9069\u7528\u3059\u308b\u306e\u304c\u7c21\u5358\u306a\u65b9\u6cd5\u3067\u3059\u3002 \u5f8c\u7d9a\u306e\u7573\u307f\u8fbc\u307f\u30d5\u30a3\u30eb\u30bf\u304c\u524d\u306e\u7573\u307f\u8fbc\u307f\u30d5\u30a3\u30eb\u30bf\u306e\u51fa\u529b\u3092\u64cd\u4f5c\u3059\u308b\uff08\u3044\u304f\u3064\u304b\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306e\u5b58\u5728\u6709\u7121\u3092\u793a\u3059\uff09\u305f\u3081\u3001\u305d\u308c\u3089\u306e\u89e3\u91c8\u304c\u96e3\u3057\u304f\u306a\u308a\u307e\u3059\u3002 Activation Maximization\u306e\u80cc\u666f\u306b\u3042\u308b\u30a2\u30a4\u30c7\u30a2\u306f\u30b7\u30f3\u30d7\u30eb\u3067\u3001\u30d5\u30a3\u30eb\u30bf\u51fa\u529b\u306e\u6d3b\u6027\u5316\u3092\u6700\u5927\u306b\u3059\u308b\u5165\u529b\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u3059\u306a\u308f\u3061\u4e0b\u8a18\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u3002 \\frac{\\partial ActivationMaximizationLoss}{\\partial input} \u305d\u3057\u3066\u3001\u305d\u306e\u63a8\u5b9a\u5024\u3092\u4f7f\u3063\u3066\u5165\u529b\u3092\u66f4\u65b0\u3059\u308b\u3053\u3068\u3067\u3059\u3002 ActivationMaximization \u640d\u5931\u95a2\u6570\u306f \u30d5\u30a3\u30eb\u30bf\u306e\u6d3b\u6027\u5316\u3092\u5927\u304d\u304f\u3059\u308b\u305f\u3081\u306b\u51fa\u529b\u3092\u5c0f\u3055\u304f\u3057\u307e\u3059\uff08\u52fe\u914d\u964d\u4e0b\u306e\u53cd\u5fa9\u4e2d\u306b\u640d\u5931\u3092\u6700\u5c0f\u5316\u3057\u307e\u3059\uff09\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u4e00\u7a2e\u306e\u5165\u529b\u30d1\u30bf\u30fc\u30f3\u306b\u5bfe\u3057\u3066\u6d3b\u6027\u5316\u3059\u308b\u7279\u5b9a\u306e\u30d5\u30a3\u30eb\u30bf\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001\u5165\u529b\u753b\u50cf\u5185\u306e\u76ee\u306e\u5b58\u5728\u3092\u6d3b\u6027\u5316\u3059\u308bEye\u30d5\u30a3\u30eb\u30bf\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \u4f7f\u3044\u65b9 There are two APIs exposed to perform activation maximization. visualize_activation : This is the general purpose API for visualizing activations. visualize_activation_with_losses : This is intended for research use-cases where some custom weighted losses can be minimized. See examples/ for code examples. Scenarios The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases below: Categorical Output Dense layer visualization How can we assess whether a network is over/under fitting or generalizing well? Given an input image, a CNN can classify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of what it means to be a bird? One way to answer these questions is to pose the reverse question: Generate an input image that maximizes the final Dense layer output corresponding to bird class. This can be done by pointing layer_idx to final Dense layer, and setting filter_indices to the desired output category. For multi-class classification, filter_indices can point to a single class. You could point also point it to multiple categories to see what a cat-fish might look like, as an example. For multi-label classifier, simply set the appropriate filter_indices . Regression Output Dense layer visualization Unlike class activation visualizations, for regression outputs, we could visualize input that increases decreases the regressed filter_indices output. For example, if you trained an apple counter model, increasing the regression output should correspond to more apples showing up in the input image. Similarly one could decrease the current output. This can be achieved by using grad_modifier option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. By default, ActivationMaximization loss is used to increase the output. By setting grad_modifier='negate' you can negate the gradients, thus causing output values to decrease. gradient_modifiers are very powerful and show up in other visualization APIs as well. Conv filter visualization By pointing layer_idx to Conv layer, you can visualize what pattern activates a filter. This might help you discover what a filter might be computing. Here, filter_indices refers to the index of the Conv filter within the layer. Advanced usage backprop_modifiers allow you to modify the backpropagation behavior. For examples, you could tweak backprop to only propagate positive gradients by using backprop_modifier='relu' . This parameter also accepts a function and can be used to implement your crazy research idea :) Tips and tricks If you get garbage visualization, try setting verbose=True to see various losses during gradient descent iterations. By default, visualize_activation uses TotalVariation and LpNorm regularization to enforce natural image prior. It is very likely that you would see ActivationMaximization Loss bounce back and forth as they are dominated by regularization loss weights. Try setting all weights to zero and gradually try increasing values of total variation weight. To get sharper looking images, use Jitter input modifier. Regression models usually do not provide enough gradient information to generate meaningful input images. Try seeding the input using seed_input and see if the modifications to the input make sense. Consider submitting a PR to add more tips and tricks that you found useful.","title":"Activation Maximization"},{"location":"visualizations/activation_maximization/#activation-maximization","text":"CNN\u3067\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306b\u306f\u3001\u5165\u529b\u753b\u50cf\u306b\u3088\u304f\u4f3c\u305f\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d1\u30bf\u30fc\u30f3\u3092\u5165\u529b\u3059\u308b\u3068\u3001\u305d\u306e\u51fa\u529b\u3092\u6700\u5927\u5316\u3059\u308b\u3044\u304f\u3064\u304b\u306e\u5b66\u7fd2\u6e08\u307f \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30de\u30c3\u30c1\u30f3\u30b0 \u30d5\u30a3\u30eb\u30bf\u304c\u3042\u308a\u307e\u3059\u3002 \u6700\u521d\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306f\u7c21\u5358\u306b\u89e3\u91c8\u3067\u3001\u5358\u306b\u91cd\u307f\u3092\u753b\u50cf\u3068\u3057\u3066\u53ef\u8996\u5316\u3059\u308b\u3060\u3051\u3067\u3059\u3002 \u7573\u307f\u8fbc\u307f\u5c64\u304c\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u898b\u308b\u305f\u3081\u306b\u306f\u3001\u5165\u529b\u30d4\u30af\u30bb\u30eb\u306b\u30d5\u30a3\u30eb\u30bf\u3092\u9069\u7528\u3059\u308b\u306e\u304c\u7c21\u5358\u306a\u65b9\u6cd5\u3067\u3059\u3002 \u5f8c\u7d9a\u306e\u7573\u307f\u8fbc\u307f\u30d5\u30a3\u30eb\u30bf\u304c\u524d\u306e\u7573\u307f\u8fbc\u307f\u30d5\u30a3\u30eb\u30bf\u306e\u51fa\u529b\u3092\u64cd\u4f5c\u3059\u308b\uff08\u3044\u304f\u3064\u304b\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306e\u5b58\u5728\u6709\u7121\u3092\u793a\u3059\uff09\u305f\u3081\u3001\u305d\u308c\u3089\u306e\u89e3\u91c8\u304c\u96e3\u3057\u304f\u306a\u308a\u307e\u3059\u3002 Activation Maximization\u306e\u80cc\u666f\u306b\u3042\u308b\u30a2\u30a4\u30c7\u30a2\u306f\u30b7\u30f3\u30d7\u30eb\u3067\u3001\u30d5\u30a3\u30eb\u30bf\u51fa\u529b\u306e\u6d3b\u6027\u5316\u3092\u6700\u5927\u306b\u3059\u308b\u5165\u529b\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u3059\u306a\u308f\u3061\u4e0b\u8a18\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u3002 \\frac{\\partial ActivationMaximizationLoss}{\\partial input} \u305d\u3057\u3066\u3001\u305d\u306e\u63a8\u5b9a\u5024\u3092\u4f7f\u3063\u3066\u5165\u529b\u3092\u66f4\u65b0\u3059\u308b\u3053\u3068\u3067\u3059\u3002 ActivationMaximization \u640d\u5931\u95a2\u6570\u306f \u30d5\u30a3\u30eb\u30bf\u306e\u6d3b\u6027\u5316\u3092\u5927\u304d\u304f\u3059\u308b\u305f\u3081\u306b\u51fa\u529b\u3092\u5c0f\u3055\u304f\u3057\u307e\u3059\uff08\u52fe\u914d\u964d\u4e0b\u306e\u53cd\u5fa9\u4e2d\u306b\u640d\u5931\u3092\u6700\u5c0f\u5316\u3057\u307e\u3059\uff09\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u4e00\u7a2e\u306e\u5165\u529b\u30d1\u30bf\u30fc\u30f3\u306b\u5bfe\u3057\u3066\u6d3b\u6027\u5316\u3059\u308b\u7279\u5b9a\u306e\u30d5\u30a3\u30eb\u30bf\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001\u5165\u529b\u753b\u50cf\u5185\u306e\u76ee\u306e\u5b58\u5728\u3092\u6d3b\u6027\u5316\u3059\u308bEye\u30d5\u30a3\u30eb\u30bf\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002","title":"Activation Maximization\u3068\u306f"},{"location":"visualizations/activation_maximization/#_1","text":"There are two APIs exposed to perform activation maximization. visualize_activation : This is the general purpose API for visualizing activations. visualize_activation_with_losses : This is intended for research use-cases where some custom weighted losses can be minimized. See examples/ for code examples.","title":"\u4f7f\u3044\u65b9"},{"location":"visualizations/activation_maximization/#scenarios","text":"The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases below:","title":"Scenarios"},{"location":"visualizations/activation_maximization/#categorical-output-dense-layer-visualization","text":"How can we assess whether a network is over/under fitting or generalizing well? Given an input image, a CNN can classify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of what it means to be a bird? One way to answer these questions is to pose the reverse question: Generate an input image that maximizes the final Dense layer output corresponding to bird class. This can be done by pointing layer_idx to final Dense layer, and setting filter_indices to the desired output category. For multi-class classification, filter_indices can point to a single class. You could point also point it to multiple categories to see what a cat-fish might look like, as an example. For multi-label classifier, simply set the appropriate filter_indices .","title":"Categorical Output Dense layer visualization"},{"location":"visualizations/activation_maximization/#regression-output-dense-layer-visualization","text":"Unlike class activation visualizations, for regression outputs, we could visualize input that increases decreases the regressed filter_indices output. For example, if you trained an apple counter model, increasing the regression output should correspond to more apples showing up in the input image. Similarly one could decrease the current output. This can be achieved by using grad_modifier option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. By default, ActivationMaximization loss is used to increase the output. By setting grad_modifier='negate' you can negate the gradients, thus causing output values to decrease. gradient_modifiers are very powerful and show up in other visualization APIs as well.","title":"Regression Output Dense layer visualization"},{"location":"visualizations/activation_maximization/#conv-filter-visualization","text":"By pointing layer_idx to Conv layer, you can visualize what pattern activates a filter. This might help you discover what a filter might be computing. Here, filter_indices refers to the index of the Conv filter within the layer.","title":"Conv filter visualization"},{"location":"visualizations/activation_maximization/#advanced-usage","text":"backprop_modifiers allow you to modify the backpropagation behavior. For examples, you could tweak backprop to only propagate positive gradients by using backprop_modifier='relu' . This parameter also accepts a function and can be used to implement your crazy research idea :)","title":"Advanced usage"},{"location":"visualizations/activation_maximization/#tips-and-tricks","text":"If you get garbage visualization, try setting verbose=True to see various losses during gradient descent iterations. By default, visualize_activation uses TotalVariation and LpNorm regularization to enforce natural image prior. It is very likely that you would see ActivationMaximization Loss bounce back and forth as they are dominated by regularization loss weights. Try setting all weights to zero and gradually try increasing values of total variation weight. To get sharper looking images, use Jitter input modifier. Regression models usually do not provide enough gradient information to generate meaningful input images. Try seeding the input using seed_input and see if the modifications to the input make sense. Consider submitting a PR to add more tips and tricks that you found useful.","title":"Tips and tricks"},{"location":"visualizations/class_activation_maps/","text":"What is a Class Activation Map? Class activation maps or grad-CAM is another way of visualizing attention over input. Instead of using gradients with respect to output (see saliency ), grad-CAM uses penultimate (pre Dense layer) Conv layer output. The intuition is to use the nearest Conv layer to utilize spatial information that gets completely lost in Dense layers. In keras-vis, we use grad-CAM as its considered more general than Class Activation maps . Usage There are two APIs exposed to visualize grad-CAM and are almost identical to saliency usage . visualize_cam : This is the general purpose API for visualizing grad-CAM. visualize_cam_with_losses : This is intended for research use-cases where some custom weighted loss can be used. The only notable addition is the penultimate_layer_idx parameter. This can be used to specify the pre-layer whose output gradients are used. By default, keras-vis will search for the nearest layer with filters. Scenarios See saliency scenarios . Everything is identical expect the added penultimate_layer_idx param. Gotchas grad-CAM only works well if the penultimate layer is close to the layer being visualized. This also applies to Conv filter visualizations. You are better off using saliency of this is not the case with your model.","title":"Class Activation Maps"},{"location":"visualizations/class_activation_maps/#what-is-a-class-activation-map","text":"Class activation maps or grad-CAM is another way of visualizing attention over input. Instead of using gradients with respect to output (see saliency ), grad-CAM uses penultimate (pre Dense layer) Conv layer output. The intuition is to use the nearest Conv layer to utilize spatial information that gets completely lost in Dense layers. In keras-vis, we use grad-CAM as its considered more general than Class Activation maps .","title":"What is a Class Activation Map?"},{"location":"visualizations/class_activation_maps/#usage","text":"There are two APIs exposed to visualize grad-CAM and are almost identical to saliency usage . visualize_cam : This is the general purpose API for visualizing grad-CAM. visualize_cam_with_losses : This is intended for research use-cases where some custom weighted loss can be used. The only notable addition is the penultimate_layer_idx parameter. This can be used to specify the pre-layer whose output gradients are used. By default, keras-vis will search for the nearest layer with filters.","title":"Usage"},{"location":"visualizations/class_activation_maps/#scenarios","text":"See saliency scenarios . Everything is identical expect the added penultimate_layer_idx param.","title":"Scenarios"},{"location":"visualizations/class_activation_maps/#gotchas","text":"grad-CAM only works well if the penultimate layer is close to the layer being visualized. This also applies to Conv filter visualizations. You are better off using saliency of this is not the case with your model.","title":"Gotchas"},{"location":"visualizations/saliency/","text":"What is Saliency? Suppose that all the training images of bird class contains a tree with leaves. How do we know whether the CNN is using bird-related pixels, as opposed to some other features such as the tree or leaves in the image? This actually happens more often than you think and you should be especially suspicious if you have a small training set. Saliency maps was first introduced in the paper: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps The idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us how output category value changes with respect to a small change in input image pixels. All the positive values in the gradients tell us that a small change to that pixel will increase the output value. Hence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention. The idea behind saliency is pretty simple in hindsight. We compute the gradient of output category with respect to input image. \\frac{\\partial output}{\\partial input} This should tell us how the output value changes with respect to a small change in inputs. We can use these gradients to highlight input regions that cause the most change in the output. Intuitively this should highlight salient image regions that most contribute towards the output. Usage There are two APIs exposed to visualize saliency. visualize_saliency : This is the general purpose API for visualizing saliency. visualize_saliency_with_losses : This is intended for research use-cases where some custom weighted loss can be used. See examples/ for code examples. Scenarios The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases below: Categorical Dense layer visualization By setting layer_idx to final Dense layer, and filter_indices to the desired output category, we can visualize parts of the seed_input that contribute most towards activating the corresponding output nodes, For multi-class classification, filter_indices can point to a single class. For multi-label classifier, simply set the appropriate filter_indices . Regression Dense layer visualization For regression outputs, we could visualize attention over input that increases decreases maintains the regressed filter_indices output. For example, consider a self driving model with continuous regression steering output. One could visualize parts of the seed_input that contributes towards increase, decrease or maintenance of predicted output. By default, saliency tells us how to increase the output activations. For the self driving car case, this only tells us parts of the input image that contribute towards steering angle increase. Other use cases can be visualized by using grad_modifier option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. To visualize decrease in output, use grad_modifier='negate' . By default, ActivationMaximization loss yields positive gradients for inputs regions that increase the output. By setting grad_modifier='negate' you can treat negative gradients (which indicate the decrease) as positive and therefore visualize decrease use case. To visualize what contributed to the predicted output, we want to consider gradients that have very low positive or negative values. This can be achieved by performing grads = abs(1 / grads) to magnifies small gradients. Equivalently, you can use grad_modifier='small_values' , which does the same thing. gradient_modifiers are very powerful and show up in other visualization APIs as well. You can see a practical application for this in the self diving car example. Guided / rectified saliency Zieler et al. has the idea of clipping negative gradients in the backprop step. i.e., only propagate positive gradient information that communicates the increase in output. We call this rectified or deconv saliency. Details can be found in the paper: Visualizing and Understanding Convolutional Networks . In guided saliency, the backprop step is modified to only propagate positive gradients for positive activations. For details see the paper: String For Simplicity: The All Convolutional Net . For both these cases, we can use backprop_modifier='relu' and backprop_modifier='guided' respectively. You can also implement your own backprop_modifier to try your crazy research idea :) Conv filter saliency By pointing layer_idx to Conv layer, you can visualize parts of the image that influence the filter. This might help you discover what a filter cares about. Here, filter_indices refers to the index of the Conv filter within the layer.","title":"Saliency Maps"},{"location":"visualizations/saliency/#what-is-saliency","text":"Suppose that all the training images of bird class contains a tree with leaves. How do we know whether the CNN is using bird-related pixels, as opposed to some other features such as the tree or leaves in the image? This actually happens more often than you think and you should be especially suspicious if you have a small training set. Saliency maps was first introduced in the paper: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps The idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us how output category value changes with respect to a small change in input image pixels. All the positive values in the gradients tell us that a small change to that pixel will increase the output value. Hence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention. The idea behind saliency is pretty simple in hindsight. We compute the gradient of output category with respect to input image. \\frac{\\partial output}{\\partial input} This should tell us how the output value changes with respect to a small change in inputs. We can use these gradients to highlight input regions that cause the most change in the output. Intuitively this should highlight salient image regions that most contribute towards the output.","title":"What is Saliency?"},{"location":"visualizations/saliency/#usage","text":"There are two APIs exposed to visualize saliency. visualize_saliency : This is the general purpose API for visualizing saliency. visualize_saliency_with_losses : This is intended for research use-cases where some custom weighted loss can be used. See examples/ for code examples.","title":"Usage"},{"location":"visualizations/saliency/#scenarios","text":"The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases below:","title":"Scenarios"},{"location":"visualizations/saliency/#categorical-dense-layer-visualization","text":"By setting layer_idx to final Dense layer, and filter_indices to the desired output category, we can visualize parts of the seed_input that contribute most towards activating the corresponding output nodes, For multi-class classification, filter_indices can point to a single class. For multi-label classifier, simply set the appropriate filter_indices .","title":"Categorical Dense layer visualization"},{"location":"visualizations/saliency/#regression-dense-layer-visualization","text":"For regression outputs, we could visualize attention over input that increases decreases maintains the regressed filter_indices output. For example, consider a self driving model with continuous regression steering output. One could visualize parts of the seed_input that contributes towards increase, decrease or maintenance of predicted output. By default, saliency tells us how to increase the output activations. For the self driving car case, this only tells us parts of the input image that contribute towards steering angle increase. Other use cases can be visualized by using grad_modifier option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. To visualize decrease in output, use grad_modifier='negate' . By default, ActivationMaximization loss yields positive gradients for inputs regions that increase the output. By setting grad_modifier='negate' you can treat negative gradients (which indicate the decrease) as positive and therefore visualize decrease use case. To visualize what contributed to the predicted output, we want to consider gradients that have very low positive or negative values. This can be achieved by performing grads = abs(1 / grads) to magnifies small gradients. Equivalently, you can use grad_modifier='small_values' , which does the same thing. gradient_modifiers are very powerful and show up in other visualization APIs as well. You can see a practical application for this in the self diving car example.","title":"Regression Dense layer visualization"},{"location":"visualizations/saliency/#guided-rectified-saliency","text":"Zieler et al. has the idea of clipping negative gradients in the backprop step. i.e., only propagate positive gradient information that communicates the increase in output. We call this rectified or deconv saliency. Details can be found in the paper: Visualizing and Understanding Convolutional Networks . In guided saliency, the backprop step is modified to only propagate positive gradients for positive activations. For details see the paper: String For Simplicity: The All Convolutional Net . For both these cases, we can use backprop_modifier='relu' and backprop_modifier='guided' respectively. You can also implement your own backprop_modifier to try your crazy research idea :)","title":"Guided / rectified saliency"},{"location":"visualizations/saliency/#conv-filter-saliency","text":"By pointing layer_idx to Conv layer, you can visualize parts of the image that influence the filter. This might help you discover what a filter cares about. Here, filter_indices refers to the index of the Conv filter within the layer.","title":"Conv filter saliency"}]}